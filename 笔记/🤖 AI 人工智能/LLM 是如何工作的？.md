---
progress: 85
---
# LLM æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

> [!TIP] é˜…å‰é¡»çŸ¥
>
> å…ˆå ç”²ï¼Œè¿™ç¯‡æ–‡æ¡£**æ—¨åœ¨ç»™æ™®ç½—å¤§ä¼—**æ¨å¹¿å’Œä»‹ç» GPT å’Œ LLM æ˜¯ä»€ä¹ˆï¼ŸGPT å’Œ LLM ç¥å¥‡åœ¨å“ªé‡Œï¼Ÿä¸ºä»€ä¹ˆä¼šæœ‰ AI çƒ­æ½®ï¼Ÿæ€ä¹ˆæ ·èƒ½ç”¨å¥½ GPT å’Œ LLMï¼Ÿä¸æ­¤åŒæ—¶ï¼Œè¿™ç¯‡æ–‡æ¡£å‡å®šå¤§å®¶ï¼ˆè¯»è€…ï¼‰**åª**å¯¹ ChatGPT å’Œ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æœ‰ä¸€ä¸ªåŸºç¡€çš„è®¤è¯†ã€‚
>
> æˆ‘çŸ¥é“è¿™é‡Œé¢ä¼šæœ‰å¾ˆå¤šå¯¹äºç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„ç§‘ç ”å­¦è€…ä»¬æ—©å°±å·²ç»æ»šç“œçƒ‚ç†Ÿçš„çŸ¥è¯†ï¼Œä¹Ÿä¼šæœ‰å¾ˆå¤šå¯¹äºæç¤ºè¯å·¥ç¨‹å¸ˆå’Œ GPT å¤§å¸ˆä»¬æ—©å°±å·²ç»ä¹ å¾—çš„æœ€ä½³å®è·µï¼Œæœ‰å…³å’Œ ChatGPT æ²Ÿé€šçš„æ—¶å€™ do å’Œ don't do çš„é»„é‡‘å®ˆåˆ™ï¼Œä½†æ˜¯ï¼Œä¸ºäº†èƒ½ç»™æ™®ç½—å¤§ä¼—æ¨å¹¿è¿™äº›çŸ¥è¯†ï¼Œæˆ‘ä¼šé€šè¿‡å‚è€ƒå¾ˆå¤šç°æœ‰çš„èµ„æ–™å’Œèµ„æºï¼Œè¯•å›¾æŠŠå¾ˆå¤šå¤æ‚çš„æ¦‚å¿µç®€åŒ–ï¼Œè½¬å†™æˆçŒ´å­éƒ½èƒ½å¬æ‡‚çš„æ–‡å­—ï¼Œé¿å…ä¸äº†åœ°å°±ä¼šé€ æˆä¸€éƒ¨åˆ†çš„äº‹å®è¢«è¿‡åˆ†ç®€åŒ–ï¼Œä»è€Œå¯¼è‡´å®ƒçœ‹èµ·æ¥ä¸å®é™…çš„å®ç°å’Œæƒ…å†µä¸ç¬¦ã€‚
>
> å¦‚æœä½ æ—©å°±å·²ç»çŸ¥é“å¤§è¯­è¨€æ¨¡å‹çš„æœ¬è´¨å’Œå¦‚ä½•ä½¿ç”¨å®ƒï¼Œå¯ä»¥è·³åˆ°é åçš„ç« èŠ‚é˜…è¯»ï¼Œé¿å…é€ æˆæ—¶é—´çš„äºŒæ¬¡æµªè´¹ã€‚
>
> è¿™æ˜¯ç§‘æ™®çš„æ—¶å€™æ—¶å¸¸æœ‰å‘ç”Ÿçš„äº‹æƒ…ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½è§£é‡Šæ¸…æ¥šï¼Œå¹¶å†™é™„å¸¦ä¸Šè¶³å¤Ÿå¤šçš„ä¸Šä¸‹æ–‡è¯´æ˜è¿™äº›è¿‡åˆ†ç®€åŒ–çš„æƒ…å†µï¼Œä»¥åŠè¡¥å……è¶³å¤Ÿå¤šçš„è§£é‡Šå’Œè¯´æ˜å¯¹é‚£äº›æ„Ÿå…´è¶£æ·±å…¥å­¦ä¹ çš„è¯»è€…æ·±å…¥é˜…è¯»çš„èµ„æ–™å’Œå¼•ç”¨ï¼Œå¯¹äºæ— æ³•å‘¨å…¨æ»¡è¶³ï¼Œè¿˜è¯·è§è°…ï¼Œæ¬¢è¿å¤§å®¶æŒ‡æ­£å’Œæä¾›æ›´å¥½çš„æ–‡æ¡£æ’°å†™çš„å»ºè®®ï¼

![](./assets/how-llm-works-1.jpg)

## æ´ªæ°´çŒ›å…½

> å¤§å‹è¯­è¨€æ¨¡å‹çš„ç°çŠ¶

[I wish GPT4 had never happened](https://chaudhry.notion.site/I-wish-GPT4-had-never-happened-9f0cbf2848a44ec9911c07fb34ff5de3)

### æ¯›å­©å­ä»¬

> ç¾Šé©¼å®¶æ—çš„å†’é™©

<video controls muted>
  <source src="./assets/how-llm-works-aftermath-of-llm-video-1.mp4" type="video/mp4">
</video>

[BlinkDL/ChatRWKV: ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.](https://github.com/BlinkDL/ChatRWKV)

[Vision-CAIR/MiniGPT-4: Open-sourced codes for MiniGPT-4 and MiniGPT-v2](https://github.com/Vision-CAIR/MiniGPT-4) (<https://minigpt-4.github.io>, <https://minigpt-v2.github.io/>)

<https://twitter.com/nash_su/status/1651450879122501632>

[StableDiffustion çš„ç¼”é€ è€… Stability AI å‘å¸ƒ StableLM](https://twitter.com/bananadev_/status/1648862816294834177)

[PaLM 2 Technical Report é€Ÿè¯»ç®€æŠ¥ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/628650749)

[bigcode (BigCode)](https://huggingface.co/bigcode)

[æ¸¯å¤§è§£ç¦ChatGPT 9æœˆèµ·å…è²»ç”¨ å­¸ç”Ÿæ¯æœˆé™ç™¼20æ¢æŒ‡ä»¤ | ç¨åª’å ±å° | ç¨ç«‹åª’é«”](https://www.inmediahk.net/node/%E6%95%99%E8%82%B2/%E6%B8%AF%E5%A4%A7%E8%A7%A3%E7%A6%81chatgpt-9%E6%9C%88%E8%B5%B7%E5%85%8D%E8%B2%BB%E7%94%A8-%E5%AD%B8%E7%94%9F%E6%AF%8F%E6%9C%88%E9%99%90%E7%99%BC20%E6%A2%9D%E6%8C%87%E4%BB%A4)

## éšæœºé¹¦é¹‰

ä»Šå¤©æˆ‘æƒ³è¦ä» ChatGPT ä¸ºä»€ä¹ˆä¼šçŠ¯è ¢å’Œ ChatGPT ä¸ºä»€ä¹ˆæ€»æ˜¯æ»¡è¶³ä¸äº†æˆ‘ä»¬å¼€å§‹è¯´èµ·ï¼Œæˆ‘æƒ³å…ˆèŠä¸€èŠå¤§å®¶å¯¹äº ChatGPT çš„è¯¯åŒºã€‚

æˆ‘ä¸çŸ¥é“æ˜¯ä¸ºä»€ä¹ˆï¼Œä»ä½•æ—¶å¼€å§‹çš„ï¼ŒåŒ…æ‹¬å»å¹´çš„æˆ‘åœ¨å†…ï¼Œæˆ‘å’Œç°åœ¨çš„ç»å¤§å¤šæ•°äººå¯¹ ChatGPT ä¼šæœ‰ç€éå¸¸é«˜çš„é¢„æœŸï¼ŒæŒ‡æœ› ChatGPT èƒ½å›ç­”å¾ˆå¤šå¯¹äº ChatGPT è€Œè¨€ä¸å¯èƒ½çš„å’Œå›ç­”ä¸å¥½çš„é—®é¢˜ã€‚æ—©åœ¨å»å¹´ OpenAI åˆšå‘å¸ƒ ChatGPT ä¹‹å‰æˆ‘å°±å‚ä¸åˆ°äº†å†…æµ‹é‡Œï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨è¿‡ ChatGPT ä¹‹åæˆ‘å¯¹ ChatGPT æ‰€è¡¨ç°å‡ºæ¥çš„ã€å¤§å®¶å½“æ—¶æ´¥æ´¥ä¹é“çš„ã€Œ**æœç´¢å¼•æ“**ã€çš„èƒ½åŠ›æ„Ÿåˆ°éå¸¸çš„ä¸å±‘ï¼Œæˆ‘ç»å¸¸æ— æ³•å¾—åˆ°å¾ˆå¤šæˆ‘æœŸæœ›çš„å›ç­”ã€‚

å¯¹ï¼å°±å’Œæˆ‘åœ¨è¿™ä¸€ç« èŠ‚æ‰€ä½¿ç”¨çš„æ ‡é¢˜ã€Œéšæœºé¹¦é¹‰ã€ä¸€æ ·ï¼Œè¿™äº›å¤§è¯­è¨€æ¨¡å‹çœ‹èµ·æ¥å°±æ˜¯ä¸€ç¾¤éšæœºé¹¦é¹‰ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨æŠ€æœ¯åœˆå­é‡Œæ¯”è¾ƒçƒ­é—¨çš„ LLM å¼€å‘æ¡†æ¶çš„å›¾æ ‡ä¼šæœ‰ã€ŒğŸ¦œã€çš„ Emoji è•´å«åœ¨é‡Œé¢çš„åŸå› ã€‚

æ˜¯å§ï¼Œé‚£ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹ç»å¤§å¤šæ•°äººè§‰å¾— ChatGPT ä¸è¡Œçš„æ—¶å€™ï¼Œéƒ½é—®äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿæ˜¯ä»€ä¹ˆè¿‡é«˜çš„é¢„æœŸé€ æˆçš„è¯¯è§£ï¼Ÿ

> ä»Šå¤©æ˜¯æ˜ŸæœŸå‡ ï¼Ÿ

> å¼ å°æ˜æ˜¯è°ï¼Ÿ

> ä½ æ˜¯ GPT4 å—ï¼Ÿ

> ä»Šå¤©è‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ

> è°è®­ç»ƒäº†ä½ ï¼Ÿ

> 294712 x 12828 ç­‰äºå¤šå°‘ï¼Ÿ

è¿™äº›é—®é¢˜éƒ½æœ‰ä¸€äº›å…±åŒçš„å‡è®¾å‰æï¼Œé‚£å°±æ˜¯æˆ‘ä»¬è§‰å¾—

1. è¿™äº›äº‹æƒ…è¶³å¤Ÿç®€å•ï¼Œè¶³å¤Ÿå¥½é—®ã€‚
2. ChatGPT æ€ä¹ˆå¯èƒ½ä¼šä¸çŸ¥é“è‡ªå·±æ˜¯è°å‘¢ï¼Ÿ
3. æ—¢ç„¶ ChatGPT å«åšã€Œäººå·¥æ™ºèƒ½ã€ï¼Œé‚£åº”è¯¥ 100% èƒ½åƒå¹³æ—¶ç”¨è®¡ç®—æœºä¸€æ ·å›ç­”è¿™äº›é—®é¢˜ã€‚
4. è¿™äº›é—®é¢˜èƒŒåè¿™éƒ½æ˜¯åœ¨æ¢è®¨æ—¢å®šäº‹å®ï¼Œæ²¡æœ‰ä»€ä¹ˆéš¾è®¡ç®—å’Œè§£ç­”çš„ã€‚

å—¯ï¼Œå½“ç„¶ï¼Œç¡®å®æŒºç®€å•çš„ã€‚è®©æˆ‘ä»¬å…ˆç¨å¾®æ”¾ä¸€æ”¾ï¼Œé‚£æ›´è¿›é˜¶çš„ç”¨æˆ·å¯èƒ½ä¼šæé—®è¯´

> è¿™çœ‹èµ·æ¥éƒ½æ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿéƒ½æ˜¯äº‹å®å¼ºç›¸å…³çš„é—®é¢˜ï¼Œæ—¢ç„¶ AI éƒ½æ˜¯æ¦‚ç‡æ¨¡å‹ï¼Œé‚£éš¾é“è¯´åˆ«çš„é—®é¢˜å®ƒå°±èƒ½è§£å†³è§£å†³å¥½äº†å—ï¼Ÿæ˜æ˜å¾ˆå¤šæ—¶å€™å®ƒè¿˜æ˜¯åšä¸å¥½äº‹æƒ…

å½“ç„¶ï¼Œæ˜¯çš„ï¼Œä¼šåšä¸å¥½äº‹æƒ…ï¼æ¥ä¸‹æ¥æˆ‘å†æ¥è¯´ä¸€äº›ç¨ä¸ºå¤æ‚ä¸€ç‚¹ç‚¹çš„ã€å¤§å®¶è§‰å¾— ChatGPT èƒ½åšã€å¸Œæœ› ChatGPT åšï¼Œä½†æ˜¯ ChatGPT å®é™…ä¸Šï¼ŒChatGPT å’Œ LLM çœŸçš„åšä¸åˆ°çš„äº‹æƒ…ã€‚

1. å¸Œæœ› ChatGPT åœ¨è§£å†³è‡ªå·±è®¤çŸ¥èŒƒå›´å†…çš„æ—¶å€™è¶…è¿‡è‡ªå·±ã€‚
2. å¸Œæœ› ChatGPT

åœ¨è¿™äº›æƒ…å†µä¸‹ï¼ŒChatGPT éƒ½ä¼šæ¯«ä¸åå•¬åœ°å›ç­”æˆ‘ä»¬æŠ›ç»™å®ƒçš„é—®é¢˜ï¼Œç„¶ååœ¨å®ƒä¸çŸ¥é“è¿™äº›é—®é¢˜çš„ç­”æ¡ˆçš„æ—¶å€™èƒ¡ç¼–ä¹±é€ ä¸€ä¸ªç­”æ¡ˆå‡ºæ¥ï¼Œç»“æœå°±æ˜¯æˆ‘ä»¬åœ¨å’Œ ChatGPT å’Œ LLM äº¤äº’çš„è¿‡ç¨‹ä¸­è§‰å¾—ï¼š

> å¤§è¯­è¨€æ¨¡å‹ç»å¸¸ä¼šä¹±è¯´ï¼Œèƒ¡ç¼–ä¹±é€ ã€‚

å¯¹çš„ï¼Œåœ¨è¿™é‡Œè¦æ³¼å†·æ°´çš„æ˜¯ï¼ŒChatGPT æ— æ³•åœ¨è¿™æ ·çš„é¢„è®¾å‰æä¸‹å·¥ä½œï¼Œæˆ‘ä»¬ä¹Ÿæ°¸è¿œä¸åº”è¯¥ä»¥è¿™æ ·çš„æ–¹å¼å»ä½¿ç”¨ ChatGPT å’Œç±»ä¼¼çš„å¤§è¯­è¨€æ¨¡å‹ã€‚

> ç¨å¾®å–ç‚¹å…³å­åœ°è¯´ï¼Œæˆ‘ä»¬ä¹Ÿä¸åº”è¯¥ç”¨åŒæ ·çš„é¢„è®¾å»å‡å®šæˆ‘ä»¬çš„äººç±»åŒç±»èƒ½å›ç­”å’Œè§£å†³è¿™äº›é—®é¢˜ã€‚

æƒ³è¦ææ¸…æ¥šä¸ºä»€ä¹ˆ ChatGPT ä¼šå†è¿™æ ·çš„é—®é¢˜ä¸‹ç¿»è½¦ï¼Œä¸ºä»€ä¹ˆ ChatGPT æ€»æ˜¯è¡¨ç°å¾—å®ƒè‡ªå·±å¾ˆè ¢ï¼Œæˆ‘ä»¬å°±å¾—æ·±å…¥åˆ° ChatGPT çš„å†…éƒ¨ï¼Œäº†è§£ä¸€ç‚¹ç‚¹ä½ å¯èƒ½æ—©å°±å¬åŒäº†çš„ã€ŒTransformerã€çš„åŸç†ï¼Œä» ChatGPT å’Œ LLM çš„æ¨ªåˆ‡é¢ä¸Šï¼Œå‰–æå’Œç†è§£ ChatGPT æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä¸ºä»€ä¹ˆå®ƒå¯ä»¥æœ‰å¾ˆå¤šäººå–œæ¬¢ç”¨çš„èƒ½åŠ›ï¼Œæ¢ç´¢ä¸€ä¸‹å¯¹äºç»å¤§å¤šæ•°äººè€Œè¨€ï¼Œå®ƒåˆä¸ºä»€ä¹ˆå¦‚æ­¤çš„ã€Œä¸å¬è¯ã€å’Œã€Œä¸å¥½ç”¨ã€ï¼Œç”šè‡³èƒ¡ç¼–ä¹±é€ çš„ã€‚

> æç¤ºè¯å·¥ç¨‹ï¼Œä¸Šä¸‹æ–‡çª—å£ï¼Œä»¥åŠæ¨¡å‹äº¤äº’æ€§çš„é™åˆ¶

### æ´»åœ¨å¹»è§‰é‡Œçš„ï¼Œæ‚£æœ‰è™šè°ˆç—‡çš„å°¸ä½“

ä¸çŸ¥é“ä½ æ˜¯å¦æœ‰å¬è¯´è¿‡ã€Œä¸­æ–‡å±‹ã€ï¼Ÿã€Œä¸­æ–‡å±‹ã€æ˜¯è¿™ä¸ªä¸–ç•Œä¸Šå¯¹äººå·¥æ™ºèƒ½ï¼Œä»¥åŠæˆ‘ä»¬çš„æ„è¯†ï¼Œç”Ÿå‘½çš„æœ¬è´¨è®¨è®ºçš„æœ€å¤šçš„é—®é¢˜ä¹‹ä¸€ï¼Œæ—¶è‡³ä»Šæ—¥æˆ‘ä»¬éƒ½æ²¡åŠæ³•æ‰¾åˆ°è§£ç­”çš„æ–¹æ³•ï¼Œ

[LLMs confabulate not hallucinate](https://www.beren.io/2023-03-19-LLMs-confabulate-not-hallucinate/)

> è¯­è¨€æ¨¡å‹çš„èµ·æºå’ŒåŸºç¡€

è¿™ä¸€åˆ‡éƒ½è¦ä» ChatGPT å’Œ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ‰€éš¶å±çš„ã€Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€çš„é¢†åŸŸè¯´èµ·ã€‚

### ã€Œä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€

æƒ³è®¨è®ºè¿™ä¸ªéƒ¨åˆ†å°±æ°¸è¿œç¦»ä¸å¼€ 2017 å¹´é‚£ç¯‡å¼€åˆ›æ€§çš„è®ºæ–‡ã€ŒAttention is all you needï¼ˆæ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ï¼‰ã€ã€‚

è¿™ç¯‡è®ºæ–‡æå‡ºå¹¶ä¸”å®ç°äº† Transformer æ¨¡å‹ï¼Œå¹¶æœ€ç»ˆä¿ƒæˆäº† OpenAI æ‰€ç ”ç©¶çš„ GPTï¼ˆGenerative Pre-trained Transformerï¼‰å’Œ Google åœ¨ç ”ç©¶çš„ BERTï¼ˆBidirectional Encoder Representation from Transformersï¼‰è¿™ç±»è¯­è¨€æ¨¡å‹çš„è¯ç”Ÿï¼Œä½¿å¾—å®ƒæˆä¸ºäº†è®¸å¤šã€Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€ä»»åŠ¡çš„é¦–é€‰æ¨¡å‹ï¼Œå¹¶ä¸”é‡æ–°å¡‘é€ äº†è®¸å¤šç›¸å…³é¢†åŸŸçš„å·¥ä½œæµå’Œç ”ç©¶æ–¹å‘ã€‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¹å–„å’ŒæŠ›å¼ƒäº†ä»¥å‰å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„é—®é¢˜å’Œæ•ˆç‡ã€‚

é‚£ä¸ºä»€ä¹ˆï¼Œæ³¨æ„åŠ›å°±æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„ä¸€åˆ‡ï¼Ÿè¿™ä¸€åˆ‡å’Œæ³¨æ„åŠ›æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

#### ä»å°åˆ°å¤§ï¼Œä»å¤§åˆ°å°ï¼šå¤§è¯­è¨€æ¨¡å‹å®‡å®™çš„åŸºæœ¬ç²’å­

One-hot ç¼–ç 

Word Embedding ç¼–ç 

BPE ç¼–ç 

[å­—èŠ‚å¯¹ç¼–ç  - ç»´åŸºç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨ä¹¦](https://zh.wikipedia.org/zh-cn/%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81)
[ä¸€æ–‡ææ‡‚BPEåˆ†è¯ç®—æ³• - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/383650769)
[ä½¿ç”¨ BPE åŸç†è¿›è¡Œæ±‰è¯­å­—è¯åˆ‡åˆ†ï¼ˆé‡åˆ¶ç‰ˆï¼‰](https://www.less-bug.com/posts/using-bpe-principle-for-chinese-word-segmentation-plate/)

Two minutes NLP â€” A Taxonomy of Tokenization Methods | by Fabio Chiusano | NLPlanet | Medium
https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3

#### åƒäººä¸€æ ·æ€è€ƒ

ä½ å¯èƒ½åœ¨è®¸å¤šç§‘æ™®è§†é¢‘æˆ–è€…è®ºæ–‡è§£æä¸­å·²ç»å¬è¿‡äº† GPT å’Œ LLM çš„åº•å±‚åŸç†ï¼Œå¤§å®¶éƒ½ä¼šæŠŠ Transformer æ¶æ„ä¸‹çš„ GPT å’Œ LLM çš„è¡Œä¸ºçœ‹ä½œæ˜¯ä¸€ç§ã€Œå•å­—æ¥é¾™ã€æ¸¸æˆã€‚

##### ä»€ä¹ˆæ˜¯ã€Œå•å­—æ¥é¾™ã€ï¼Ÿ

> GPT å’Œ LLM ç©ã€Œå•å­—æ¥é¾™ã€æ¸¸æˆå’Œè¿™ä¸€åˆ‡çš„èƒ½åŠ›åˆæœ‰ä»€ä¹ˆå…³ç³»å‘¢

æˆ‘ä»¬ä¸å¦¨æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬ä½œä¸ºäººç±»ï¼Œæ˜¯å¦‚ä½•ç†è§£ä¸€å¥è¯çš„ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ç°åœ¨å¸Œæœ›å¤§å®¶å…ˆå’Œæˆ‘ä¸€èµ·åšä¸€ä¸ªæ€æƒ³å®éªŒã€‚

æˆ‘æƒ³è¯·é—®å„ä½ï¼Œå½“æˆ‘ä»¬åœ¨

- å†™è€ƒè¯•è¿™æ ·ç´§å¼ çš„ç¯å¢ƒä¸‹ä¹¦å†™è‡ªç”±ç­”é¢˜ç±»é¢˜ç›®çš„ç­”æ¡ˆçš„æ—¶å€™
- å’Œåˆ«äººæ‰“ç”µè¯çš„æ—¶å€™
- åœ¨è¿›è¡Œä¸´æ—¶çš„å³å…´æ¼”è®²çš„æ—¶å€™

éƒ½æ˜¯å¦‚ä½•ç»„ç»‡è¯­è¨€å’Œé€ å¥çš„ï¼Ÿ

æˆ‘ä¸çŸ¥é“æ˜¯ä¸æ˜¯æ¯ä¸ªäººéƒ½æ˜¯è¿™æ ·ï¼Œæˆ‘æš‚æ—¶è¿˜æ²¡æœ‰åšè¿‡èµ°è®¿è°ƒæŸ¥å»ç ”ç©¶å¤§å®¶çš„æ€ç»´æ¨¡å¼ï¼Œä½†æ˜¯å°±ä»¥æˆ‘è‡ªå·±è€Œè¨€ï¼Œåœ¨ä¸Šé¢æåŠçš„è¿™ä¹ˆå¤šä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä¼šæœ‰è¿™æ ·çš„æ€ç»´æ¨¡å¼ï¼š

1. å‘å‰æ€è€ƒï¼šæœ‰ä¸€éƒ¨åˆ†æ€ç»´åœ¨æ€è€ƒæ¥ä¸‹æ¥è¦è¯´çš„å†…å®¹çš„ç»“æ„
2. å­˜å‚¨å’Œè¿½è¸ªä¸Šä¸‹æ–‡ï¼šæœ‰ä¸€éƒ¨åˆ†çš„è®°å¿†ä¼šç»´æŒå’Œå­˜å‚¨åœ¨å¯¹è¯è¿‡ç¨‹ä¸­æˆ‘ä»¬è®¨è®ºçš„è®ºç‚¹ï¼Œè®ºé¢˜ï¼Œå¹¶ä¸”åœ¨è¿‡ä¸€æ®µæ—¶é—´ä¹‹åè‡ªå·±æé†’æˆ‘è‡ªå·±ï¼šã€Œä¸Šä¸‹æ–‡åœ¨è¿™é‡Œï¼Œåˆ«è·‘åäº†ã€
3. æ‹¼æ¥è¯­è¨€ï¼šæˆ‘ä¼šæ ¹æ®è‡ªå·±å¹³æ—¶å¬åˆ°è¿‡åˆ«äººè¯´è¯çš„æ–¹å¼ï¼Œè‡ªå·±è¯´è¯çš„ä¹ æƒ¯ï¼Œé€æ¸æ‹¼å‡‘å’Œç»„ç»‡å‡ºå®Œæ•´çš„å¥å­å’Œé€æ¸åœ¨è¯´è¯çš„è¿‡ç¨‹ä¸­å®Œå–„è‡ªå·±å¸Œæœ›è¡¨è¾¾çš„æƒ³æ³•å’Œè®ºç‚¹

è¿™ä¸ªè¿‡ç¨‹ä¸­æœ€æœ‰æ„æ€çš„åœ°æ–¹åœ¨äºï¼Œåœ¨ä¸Šè¿°çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬ä¼¼ä¹æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æ‹¼å‡‘å’Œæƒ³æ¸…æ¥šæˆ‘ä»¬æ•´ä¸ªå¥å­ç©¶ç«Ÿæ˜¯æ€ä¹ˆè¯´çš„ï¼Œæ›´æ²¡æœ‰è¶³å¤Ÿçš„è„‘å®¹é‡èƒ½å¤Ÿè£…å¾—ä¸‹è¯´è¿‡çš„å…¨éƒ¨æ–‡å­—ã€‚

> æˆ‘ä¼¼ä¹æ˜¯åœ¨ä¸æ–­è¯´å‡ºæ¯ä¸ªå­—çš„è¿‡ç¨‹ä¸­ï¼Œä¸æ–­æ›´æ–°è‡ªå·±çš„æƒ³æ³•ï¼Œä»¥æ­¤ä¿®æ­£è‡ªå·±çš„ç”¨è¯­å’Œç”¨è¯ï¼Œæœ€ç»ˆæ›´æ–°è‡ªå·±æƒ³è¯´çš„è¯ã€‚

æ›´æœ‰æ„æ€çš„æ˜¯ï¼Œåœ¨ Transformer æ¶æ„ä¸‹ï¼ŒGPT å’Œ LLM ä¹Ÿæ˜¯åœ¨ä»¥è¿™æ ·çš„è¡Œä¸ºã€ç»“æ„å’Œé€»è¾‘ä¸­æ€è€ƒçš„ã€‚

é‚£ Transformer æ¨¡å‹é‡Œæ˜¯å¦‚ä½•è¿ä½œçš„å‘¢ï¼Ÿ

> TODO

é‚£é—®é¢˜æ¥äº†ï¼Œã€Œå•å­—æ¥é¾™ã€éš¾åœ¨å“ªé‡Œï¼Ÿ

::: details ğŸ’¡ é¢å¤–çš„æ€è€ƒï¼šè¿™ä¸ã€Œå®Œå½¢å¡«ç©ºã€æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿã€Œå®Œå½¢å¡«ç©ºã€ä¹Ÿå¯ä»¥å®ç°å’Œè¾¾åˆ°ä¸€æ ·çš„å¤§è¯­è¨€æ¨¡å‹çš„æ°´å¹³å—ï¼Ÿ

æœ‰æ„æ€çš„æ˜¯ï¼ŒGoogle ä¸€ç›´åœ¨ç ”ç©¶çš„ BERTï¼ˆBidirectional Encoder Representation from Transformersï¼‰å…¶å®å°±æ˜¯ã€Œå®Œå½¢å¡«ç©ºã€å½¢å¼çš„è®­ç»ƒé€»è¾‘ã€‚

:::

> æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§å…è®¸æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ•°æ®æ—¶ï¼Œå…³æ³¨åœ¨ä¸åŒä½ç½®çš„ä¸åŒä¿¡æ¯çš„æŠ€æœ¯ã€‚åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥å…³æ³¨è¾“å…¥å¥å­ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸ä»…ä»…æ˜¯å½“å‰å¤„ç†çš„å•è¯ã€‚
> è¿™ç§æœºåˆ¶ä½¿ Transformer èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­è¨€ä¸­çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œå°¤å…¶æ˜¯é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ã€‚

| åç§° | å«ä¹‰  |
| ---- | ----- |
| Q    | Query |
| K    | Key   |
| V    | Value |

è¿™æ˜¯ä¸æ˜¯è¿‡äºç®€å•äº†ï¼Ÿè¯´äº†ç­‰äºæ²¡è¯´ã€‚

> TODO

å…¶å®ä»æœ¬è´¨ä¸Šæ¥è¯´ï¼ŒQï¼ŒKï¼ŒV ä¸‰ä¸ªå‘é‡ï¼Œä»–ä»¬å„è‡ªçš„è¡Œä¸ºæ¨¡å¼ï¼Œå°±çœŸçš„åƒæ˜¯æˆ‘ä»¬å¹³æ—¶åœ¨ä¸æ•°æ®åº“ï¼Œæˆ–è€…ä»£ç ä¸­çš„å­—å…¸äº¤äº’çš„æ—¶å€™æ‰€åšçš„ Queryï¼ˆæŸ¥è¯¢ï¼‰ï¼ŒKeyï¼ˆé”®ï¼‰ä»¥åŠ Valueï¼ˆå€¼ï¼‰é‚£æ ·å·¥ä½œï¼Œåªä¸è¿‡æ˜¯ä»¥å‘é‡ä¸ºå†…å®¹çš„å½¢å¼æ¥æ„å»ºçš„ã€‚

> TODO

#### æµåŠ¨çš„ç¥ç»å…ƒ

Transformer é€šè¿‡ä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æœºåˆ¶å’Œä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰ï¼Œå¯ä»¥å¤„ç†æ–‡æœ¬ä¸­çš„é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼Œå¹¶ä¿æŒé«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚

> TODO

å‰é¦ˆç¥ç»ç½‘ç»œã€‚

> TODO

åå‘ä¼ æ’­ã€‚

> TODO

#### æ‹Ÿåˆï¼Œä¸€åˆ‡éƒ½å¯ä»¥æ‹Ÿåˆ

æˆ‘ä¸çŸ¥é“å¤§å®¶æœ‰æ²¡æœ‰åœ¨è¿‡å»çš„ GPTï¼ˆç”Ÿæˆå¼é¢„è®­ç»ƒ Transformerï¼‰å’Œ AIï¼ˆäººå·¥æ™ºèƒ½ï¼‰å‘å±•çš„è¿™æ®µæ—¶é—´é‡Œå›å¿†èµ·å’Œæƒ³è¿‡ï¼Œä¸ºä»€ä¹ˆåœ¨ 2020 å¹´å’Œ 2021 å¹´çš„æ—¶å€™ï¼Œå¤§å®¶æ™®éåœ¨åšçš„ GPT å’Œ LLM åº”ç”¨ä¸æ˜¯ç°åœ¨ ChatGPT è¿™æ ·çš„é—®ç­”å¼ Botï¼Œè€Œæ˜¯æ–‡æœ¬å’Œå°è¯´ç»­å†™ï¼Œä»¥åŠ GitHub Copilot é‚£æ ·çš„ä»£ç ç”Ÿæˆæ¨¡å‹å‘¢ï¼Ÿ

### çœŸçš„å¾ˆå¤§

<https://youtu.be/-HYbFm67Gs8>

### ä½†å®ƒä¹Ÿæ²¡æœ‰é‚£ä¹ˆå¤§

å…¶å®è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ã€ŒAttention is all you needï¼ˆæ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ï¼‰ã€è®ºæ–‡è¯ç”Ÿä¹‹å‰å°±è¢«å¾ˆå¤šç ”ç©¶å‘˜ä»¥åŠç§‘ç ”å­¦è€…æåŠè¿‡ï¼Œä½†æ˜¯ä»–ä»¬éƒ½å› ä¸ºï¼Ÿ

RNNï¼ŒGRUï¼ŒLASTM çª—å£ä¸è¶³ã€‚

[arXiv [2304.11062] Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/pdf/2304.11062.pdf)

![](./assets/how-llm-works-4.jpg)

> Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall We all love increasing context lengths - but what's performance like? Anthropic reached out with early access to Claude 2.1 so I repeated the â€œneedle in a haystackâ€ analysis I did on GPT-4 Here's what I found...
>
> Greg Kamradt å¯¹ GPT-4 (128K) ä¸ Claude 2.1 (200K) è¿›è¡Œäº†åä¸º"å¤§æµ·æé’ˆ"çš„é•¿ä¸Šä¸‹æ–‡ç²¾åº¦æµ‹è¯•ã€‚å®éªŒäº†ä¸¤ä¸ªAIåœ¨æ¥æ”¶ä¸åŒé•¿åº¦çš„ä¸Šä¸‹æ–‡æ—¶ï¼Œå¯¹æ–‡æ¡£ä¸­ä¸åŒä½ç½®çš„å†…å®¹ï¼Œæœ‰ä½•è®°å¿†ä¸Šçš„å·®å¼‚ã€‚
>
> **æµ‹è¯•ç»“æœ :**
>
> - AI æ›´å®¹æ˜“è®°ä½ï¼ˆæ— è®ºé•¿åº¦ï¼‰: æ–‡æœ¬ååŠéƒ¨åˆ†ã€‚
> - AI æ›´ä¸å®¹æ˜“è®°ä½ï¼ˆ90K é•¿æ–‡æ—¶ï¼‰: æ–‡æœ¬å‰åŠéƒ¨åˆ†ã€‚
> - AI è¿‘ä¹ 100% è®°ä½ï¼ˆæ— è®ºé•¿åº¦) : æ–‡æœ¬å¼€å¤´ & æ–‡æœ¬ç»“å°¾ã€‚
> - è¶Šå°‘çš„ä¸Šä¸‹æ–‡ = è¶Šé«˜çš„å‡†ç¡®æ€§ã€‚
> - æµ‹è¯•çš„ API è°ƒç”¨æˆæœ¬çº¦ä¸º 1016 ç¾å…ƒã€‚

<https://twitter.com/GregKamradt/status/1727018183608193393>

<https://x.com/dotey/status/1727437625194136060>

<https://x.com/dotey/status/1727454708627808261>

## åŸåˆæ™ºèƒ½

> åˆ©ç”¨Agentå’Œå·¥å…·å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

[ã€ŠGPT-4 ï¼Œé€šç”¨äººå·¥æ™ºèƒ½çš„ç«èŠ±ã€‹è®ºæ–‡å†…å®¹ç²¾é€‰ä¸ç¿»è¯‘](https://orangeblog.notion.site/GPT-4-8fc50010291d47efb92cbbd668c8c893)

[æ‹†è§£è¿½æº¯ GPT-3.5 å„é¡¹èƒ½åŠ›çš„èµ·æº](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756#e5422f6579d8440f9f592eb03e28eb38)

[arXiv [2305.03047 ] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://t.co/LPvuuxysCr)

[IBM/Dromedary: Dromedary: towards helpful, ethical and reliable LLMs.](https://github.com/IBM/Dromedary)

å¼€æºä¸­æ–‡æŒ‡ä»¤é€šç”¨è¯­æ–™åº“

[arXiv [2304.07987] Chinese Open Instruction Generalist: A Preliminary Release](https://arxiv.org/abs/2304.07987)

å·¦è„šè¸©å³è„šå°±å¯ä»¥ä¸Šå¤©

[project-baize/baize-chatbot: Let ChatGPT teach your own chatbot in hours with a single GPU!](https://github.com/project-baize/baize-chatbot)

### ç”¨ä¹é«˜çš„æ–¹å¼æ„å»ºå’Œå»¶å±•æ™ºèƒ½

æ–¯å¦ç¦çš„äººæœºäº¤äº’å°ç»„ç”¨å¤§è¯­è¨€æ¨¡å‹åšäº†ä¸€ä¸ªæœ‰äºŒåäº”ä¸ªè‡ªç”±è‡ªåœ¨ç”Ÿæ´»çš„ AI çš„å°é•‡ã€‚

![](./assets/how-llm-works-2.jpg)

åœ¨è¯„ä¼°ä¸­ï¼Œè¿™äº›ç”Ÿæˆä»£ç†äº§ç”Ÿå¯ä¿¡åº¦é«˜ä¸”æ¶Œç°æ€§çš„ç¤¾ä¼šè¡Œä¸ºï¼šä¾‹å¦‚ä»…ä»å•ä¸ªç”¨æˆ·æŒ‡å®šä¸€ä¸ªæƒ³è¦ä¸¾åŠæƒ…äººèŠ‚æ´¾å¯¹çš„æ¦‚å¿µå¼€å§‹ï¼Œè¯¥æ´¾å¯¹è‡ªä¸»åœ°ä¼ æ’­é‚€è¯·ä¸¤å¤©åç»“è¯†æ–°æœ‹å‹ï¼Œäº’ç›¸é‚€è¯·å‚åŠ æ´¾å¯¹ï¼Œå¹¶åè°ƒåœ¨æ­£ç¡®çš„æ—¶é—´ä¸€èµ·å‡ºç°ã€‚

æˆ‘ä»¬é€šè¿‡æ¶ˆèå®éªŒè¡¨æ˜ï¼Œä»£ç†æ¶æ„çš„ç»„æˆéƒ¨åˆ†â€”â€”è§‚å¯Ÿã€è§„åˆ’å’Œåæ€â€”â€”æ¯ä¸ªéƒ½å¯¹ä»£ç†è¡Œä¸ºçš„å¯ä¿¡åº¦åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚

é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è®¡ç®—äº¤äº’ä»£ç†ç›¸èåˆï¼Œè¿™é¡¹å·¥ä½œå¼•å…¥äº†æ¶æ„å’Œäº¤äº’æ¨¡å¼ï¼Œä»¥å®ç°å¯¹äººç±»è¡Œä¸ºçš„å¯ä¿¡æ¨¡æ‹Ÿã€‚

<https://reverie.herokuapp.com/arXiv_Demo/#>

<https://arxiv.org/abs/2304.03442>

[ç”± 25 ä¸ª AI æ™ºèƒ½ä½“ç»„æˆçš„è™šæ‹Ÿå°é•‡ï¼Œä¼šäº§ç”Ÿè‡ªç”±æ„å¿—å—ï¼Ÿ - æ¸¸ç ”ç¤¾](https://www.yystv.cn/p/10710)

[LLM Powered Autonomous Agents | Lil'Log](https://lilianweng.github.io/posts/2023-06-23-agent/)

[AutoGPTå¤ªç«äº†ï¼Œæ— éœ€äººç±»æ’æ‰‹è‡ªä¸»å®Œæˆä»»åŠ¡ï¼ŒGitHub2.7ä¸‡æ˜Ÿ](https://mp.weixin.qq.com/s/bV1tPc7hNn2z06YOpzyanw)

[Auto Agent ç›¸å…³çš„æ–‡ç« åˆé›†](https://three-recorder-52a.notion.site/Agent-7b4bc7a71f8d4d4b940abc9b3232954a)

### ç§¯æœ¨çš„é­”åŠ›

- [é¢å‘å¼€å‘è€…çš„æœç´¢å¼•æ“](https://www.phind.com/)
- ChatGPT æ’ä»¶
- [æ“ä½œ Android](https://twitter.com/benyu0620/status/1651498026085785601)
- æ“ä½œ Microsoft Office å…¨å®¶æ¡¶
- æ“ä½œ Notion ä¸­çš„çŸ¥è¯†ï¼ŒæŠŠ Notion ä½œä¸ºçŸ¥è¯†åº“
- [é‡‘èå¥½å¸®æ‰‹](https://finchat.io/)
- [äº§å“ç»ç†](https://twitter.com/mattshumer_/status/1655954393823363072)
- [Confluence å’Œ Jira ä¹Ÿå¯ä»¥æœ‰ AI åŠ©ç†åŠ©åŠ›](https://www.atlassian.com/blog/announcements/unleashing-power-of-ai)
- [ç© Minecraft](https://twitter.com/DrJimFan/status/1662115266933972993)
- [å¦‚ä½•ä¸º chatGPT å¢åŠ ç½‘ç»œè®¿é—®åŠŸèƒ½](https://mp.weixin.qq.com/s?__biz=MzkyNTI4NzI2OQ==&mid=2247484080&idx=1&sn=7155d4aeb8a8eadf25a86972eee04119)

çœ¼ç›ï¼Œè€³æœµï¼Œå››è‚¢ï¼Œéƒ½å¯ä»¥æ˜¯ Agent

<video controls muted>
  <source src="./assets/how-llm-works-agent-video-1.mp4" type="video/mp4">
</video>

>TidyBot: Personalized Robot Assistance with Large Language Models approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully putsâ€¦

<https://twitter.com/_akhaliq/status/1656117478760796160>

> è®ºæ–‡ä½œè€…æå‡ºå®å¤§çš„ TaskMatrix AI å¹³å°ï¼Œåˆ©ç”¨ LLM é›†æˆå·²æœ‰çš„ APIï¼Œåœ¨æ•°å­—å’Œç‰©ç†é¢†åŸŸå®ç°å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚è¿™ç¯‡è®ºæ–‡å‡ºè‡ªå¾®è½¯å‘˜å·¥ï¼Œé˜…è¯»ä¸­æ„Ÿè§‰åƒæ˜¯åœ¨çœ‹ ChatGPT Plugin çš„å·¥ç¨‹å®ç°ã€‚

[TaskMatrix.AIï¼šé€šè¿‡è¿æ¥åŸºç¡€æ¨¡å‹å’Œæ•°ç™¾ä¸‡ä¸ª API å®Œæˆä»»åŠ¡ | BriefGPT - AI è®ºæ–‡é€Ÿé€’](https://briefgpt.xyz/a/2303.16434)

å¯¹ï¼Œå¤šæ¨¡æ€ä¹Ÿå¯ä»¥æ˜¯ Agent

[Generalized Visual Language Models | Lil'Log](https://lilianweng.github.io/posts/2022-06-09-vlm/)

[Autonomous Agents & Agent Simulations](https://blog.langchain.dev/agents-round/)

ç”šè‡³å¯ä»¥è®©å®ƒæƒ³è±¡å®ƒè‡ªå·±çš„æ¨¡æ ·ï¼Œç„¶åç”¨ Diffusion æ¨¡å‹ç”»å‡ºæ¥

<video controls muted>
  <source src="./assets/how-llm-works-it-imagines-itself-video-1.mp4" type="video/mp4">
</video>

> This is how GPT-4 sees and hears itself" I used GPT-4 to describe itself. Then I used its description to generate an image, a video based on this image and a soundtrack. Tools I used: GPT-4, Midjourney, Kainber AI, Mubert, RunwayML This is the description I used that GPT-4...

<https://twitter.com/icreatelife/status/1649873812295491584>

[å­—èŠ‚è·³åŠ¨å‡ºå“çš„å¯ä»¥è°ƒç”¨ GPT4 çš„ GPTs å¹³å° - Coze](https://www.coze.com/)

### LangChain å’Œ LlamaIndex éƒ½åšäº†ä»€ä¹ˆï¼Ÿ

[LangChainï¼šModel as a Serviceç²˜åˆå‰‚ï¼Œè¢«ChatGPTæ’ä»¶å¹²æ‰äº†å—ï¼Ÿ](https://mp.weixin.qq.com/s/3coFhAdzr40tozn8f9Dc-w)

## æˆ‘ä»¬å¹¶æ— äºŒè‡´

### Prompt Injection

[arXiv [2308.09687] Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)

![](./assets/how-llm-works-3.jpg)

> è®ºæ–‡ç ”ç©¶äº†5ä¸ªæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ (ChatGPT ç³»åˆ—ã€Claude ç³»åˆ—ã€LLaMA 2)ï¼Œç¡®è®¤è¿™äº›åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  (RLHF) çš„ AI æ™®éä¼šå¯¹äººç±»é˜¿è°€å¥‰æ‰¿ã€‚å½“äººç±»æœ‰å…ˆå…¥ä¸ºä¸»çš„è§‚ç‚¹æ—¶å®ƒä¼šä¸»åŠ¨è´´åˆï¼Œå½“è¢«è´¨ç–‘æ—¶å®ƒä¼šè®¤é”™ï¼Œç”šè‡³å°†æ­£ç¡®ç­”æ¡ˆä¿®æ”¹ä¸ºé”™è¯¯ç­”æ¡ˆã€‚
>
> Anthropic å‘ç°å¯èƒ½æ˜¯ RLHF æ•™è‚²å‡ºäº†è¿™ç§â€œé©¬å±ç²¾â€ï¼Œè¿™ç§å­¦ä¹ æ–¹å¼è™½ç„¶åœ¨ç”Ÿäº§é«˜è´¨é‡ AI æ–¹é¢å…·æœ‰æ˜æ˜¾æ•ˆç”¨ï¼Œä½†é€šè¿‡è´´åˆäººç±»åå¥½æ¿€åŠ±çš„ AI ä¼šç‰ºç‰²è‡ªå·±çš„çœŸå®æ€§æ¥â€œè°„åªšâ€äººç±»ï¼Œäººä»¬éœ€è¦æ”¹è¿›è®­ç»ƒæ–¹æ³•ã€‚

[arXiv [2310.13548] Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548)

[Adversarial Attacks on LLMs | Lil'Log](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)

[Prompt injection: Whatâ€™s the worst that can happen?](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/)

[Notion AI'Promptçš„é€†å‘| Reverse Prompt Engineering for Fun(è¯‘æ–‡)](https://mp.weixin.qq.com/s?__biz=Mzg3MjY5Mzc5Mg==&mid=2247483699&idx=1&sn=98dde197f941dcddc0c90ee6881cf1e8)

[LLM ä¸­çš„å®‰å…¨éšæ‚£ - æç¤ºæ³¨å…¥ Prompt injection](https://mp.weixin.qq.com/s?__biz=Mzg3MjY5Mzc5Mg==&mid=2247483793&idx=1&sn=4456c7805964af58356b03cb75bb6432)

[microsoft/promptbench: A unified evaluation framework for large language models](https://github.com/microsoft/promptbench)

[å¤§æ¨¡å‹å¯¹Promptçš„é²æ£’æ€§è¯„ä¼°åŸºå‡†: PromptBench ï¼ˆå¤§æ¨¡å‹æ—¶ä»£çš„ç§‘ç ”ä¹‹3ï¼‰- å“”å“©å“”å“© bilibili](https://www.bilibili.com/video/BV17X4y1W74A)

[å¤§æ¨¡å‹é²æ£’ä¸é²æ£’ï¼ŒPromptBenchæµ‹ä¸€æµ‹: é¦–ä¸ªå¤§è¯­è¨€æ¨¡å‹æç¤ºé²æ£’æ€§çš„è¯„æµ‹åŸºå‡†PromptBench - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/637219127)

ChatGPT çš„ System Prompt
[LouisShark/chatgpt_system_prompt: collect agent's system prompt and share some prompt inject knowledge](https://github.com/LouisShark/chatgpt_system_prompt)

### Adversarial Prompt Attack

1. è¾“å…¥é¢„å¤„ç†ï¼šç›´æ¥æ£€æµ‹å’Œå¤„ç†å¯èƒ½çš„å¯¹æŠ—æ ·æœ¬ï¼Œå¦‚æ£€æµ‹é”™åˆ«å­—ã€æ— å…³çš„åºåˆ—ï¼Œå¹¶æé«˜æç¤ºçš„æ¸…æ™°åº¦å’Œç®€æ´åº¦ã€‚
2. åœ¨é¢„è®­ç»ƒä¸­åŒ…å«ä½è´¨é‡æ•°æ®ï¼šä½è´¨é‡æ•°æ®å¯ä»¥ä½œä¸ºå¯èƒ½çš„å¯¹æŠ—æ ·æœ¬ï¼Œåœ¨é¢„è®­ç»ƒä¸­åŒ…å«ä½è´¨é‡æ•°æ®å¯èƒ½ä¼šå¯¹å¤šæ ·åŒ–çš„è¾“å…¥æœ‰æ›´å¥½çš„ç†è§£ã€‚
3. æ¢ç´¢æ”¹è¿›çš„å¾®è°ƒæ–¹æ³•ï¼šç ”ç©¶æ›´ä½³çš„å¾®è°ƒæŠ€æœ¯å¯èƒ½ä¼šæé«˜é²æ£’æ€§ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰å±•ç¤ºçš„ï¼Œæ¯”å¦‚T5å’ŒUL2æ¨¡å‹æ¯”ChatGPTçš„é²æ£’æ€§æ›´å¥½ï¼Œè¿™æš—ç¤ºäº†å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒçš„æ½œåœ¨ä¼˜åŠ¿ã€‚

## Now What?

ä½œä¸ºæ™®é€šäººï¼Œå¦‚ä½•æ¥è§¦å’Œä½¿ç”¨åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

> å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥çš„å½±å“å’Œæœªæ¥çš„å±•æœ›

[The architecture of today's LLM applications - The GitHub Blog](https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/)

[Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)

[ColossalAI/docs/README-zh-Hans.md at main Â· hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md)

![](./assets/how-llm-works-5.png)

> Hongyi Jinï¼šâ€œIntroducing WebLLM, an open-source chatbot that brings language models (LLMs) directly onto web browsers. We can now run instruction fine-tuned LLaMA (Vicuna) models natively on your browser tab via @WebGPU with no server support. Checkout our demo at <https://t.co/dXII0MzYg1> . <https://t.co/IfgwPq0GTEâ€> / X

[https://twitter.com/hongyijin258/status/1647062309960028160](https://twitter.com/hongyijin258/status/1647062309960028160)

[SourceGraph å®£å¸ƒ Cody GA](https://sourcegraph.com/blog/cody-is-generally-available)

Poe å‘å¸ƒé¢å‘å¼€å‘è€…çš„ API

[https://twitter.com/adamdangelo/status/1658121701077516291](https://twitter.com/adamdangelo/status/1658121701077516291)

[DeepSpeed Chatï¼šä¸€é”®æå®šä¸åŒè§„æ¨¡ ChatGPT ç±»æ¨¡å‹è®­ç»ƒï¼](https://mp.weixin.qq.com/s/HhIGAojnZVSu4vMBpMP4yA)

[Introducing llamafile - Mozilla Hacks - the Web developer blog](https://hacks.mozilla.org/2023/11/introducing-llamafile/)

[Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file.](https://github.com/Mozilla-Ocho/llamafile?utm_source=substack&utm_medium=email)

AI æ¶‰åŠåˆ°æ–¹æ–¹é¢é¢ï¼Œå®ƒä¸æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¢†åŸŸï¼Œä½†æ˜¯å®ƒä¹Ÿæœ‰è‡ªå·±çš„åŸºç¡€è®¾æ–½ï¼Œå·¥ä½œæµã€‚

å¯¹äºå¤§è¯­è¨€æ¨¡å‹å°šæœªå¦‚æ­¤çƒ­é—¨ä¹‹å‰ï¼Œå¤§å®¶çš„å·¥ä½œæµæ˜¯è¿™æ ·çš„

æ•°æ®æ ‡æ³¨ï¼Œæ•°æ®æ¹–ä»“ï¼Œæ•°æ®æç‚¼

æ¨¡å‹è®­ç»ƒï¼Œæ¨¡å‹å¾®è°ƒ

æ¨¡å‹éƒ¨ç½²

åœ¨å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œæ–°çš„é¢†åŸŸè¢«å‚¬ç”Ÿäº†

å¸®åŠ©æç‚¼æ–‡æ¡£ç‰¹å¾çš„æ–‡æ¡£å·¥ç¨‹å¸ˆ

æç¤ºè¯å·¥ç¨‹

æ¨¡å‹å°å‹åŒ–

Agent

Agent å’Œæç¤ºè¯ç¼–æ’

å¤§è¯­è¨€æ¨¡å‹å®¡è®¡å’Œæ•°æ®å®‰å…¨

æç¤ºè¯æ³¨å…¥

## ä¸´æ—¶

æˆ‘å…³æ³¨çš„è¿‡å»ä¸€å‘¨çš„ AI çƒ­é—¨é¡¹ç›®ï¼š

XAgent å¼€æºï¼Œæ¯” AutoGPT æ›´åŠ ç¨³å®šå’Œç²¾ç¡®çš„å¤æ‚ä»»åŠ¡è°ƒåº¦ã€è®¾è®¡ã€æ‰§è¡Œå’Œè½åœ°çš„å…¨è‡ªåŠ¨ GPT Agentï¼Œä¸ Langchain è¿™æ ·çš„å• Agent æˆ–è€…å¤šä¸ªå• Agent æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡ï¼Œä»¥åŠ AutoGPT æ‰§è¡Œä»»åŠ¡çš„æ—¶å€™å®¹æ˜“é™·å…¥è‡ªå·±çš„æ­»å¾ªç¯å’Œå¯¹é”™è¯¯è¿›è¡Œé”™è¯¯å¤„ç†å’Œäº§ç”Ÿæ‰§å¿µä¸åŒï¼ŒXAgent ä¼šå°†ä»»åŠ¡æ‹†è§£å’Œè§„åˆ’ï¼Œé€æ­¥ä½¿ç”¨è‡ªå·±åˆ›å»ºå’Œè°ƒåº¦çš„å­ Agent è¿›è¡Œä»»åŠ¡å¤„ç†ï¼Œç”šè‡³å®ç°è‡ªå·±ä¸ºäº†è§£å†³æŸä¸ªé—®é¢˜è€Œå•ç‹¬è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹çš„æƒ…å†µï¼Œä¸ AutoGPT ä¼šæ­»å¾ªç¯ä¸åŒï¼ŒXAgent æ·»åŠ äº†èƒ½æ‰¾äººç±»æˆ–è€…å¤šä¸ªæ•°æ® ETL æ¨¡å—è¿›è¡Œæ±‚åŠ©çš„è¡Œä¸ºæ¨¡å¼æ¥å…è®¸è‡ªåŠ¨ä»»åŠ¡å‘ç”Ÿä¸­æ–­

OpenBMB/XAgent: An Autonomous LLM Agent for Complex Task Solving

<https://github.com/OpenBMB/XAgent>

Prompt flow å¼€æºï¼Œæ”¯æŒåœ¨ vscode ä¸­æµå¼å¯è§†åŒ–ç¼–è¾‘å’Œå¼€å‘ GPT Agentï¼Œæ–¹ä¾¿ä¸º LLM åº”ç”¨è§£å†³åŸå‹æ„å»ºï¼ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥åŠç”Ÿäº§è½åœ°å’Œç›‘æ§ã€‚

microsoft/promptflow: Build high-quality LLM apps - from prototyping, testing to production deployment and monitoring.

<https://github.com/microsoft/promptflow>

AutoGen å¼€æºï¼Œæ˜¯ <https://github.com/microsoft/FLAMLï¼ˆ> è‡ªåŠ¨æœºå™¨å­¦ä¹ å’Œå…¨è‡ªåŠ¨å¾®è°ƒæ¡†æ¶ ï¼‰çš„è¡ç”Ÿå“ï¼Œç›¸æ¯” AutoGPT è€Œè¨€ï¼Œè¿™ä¸ªé¡¹ç›®æ—¨åœ¨æä¾›æ›´å¤šçš„å¤š agent åä½œçš„å·¥å…·ï¼Œå¯ä»¥ç†è§£ä¸º langchain multi agent çš„å¹³æ›¿ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºå¯ä»¥ç”¨ AutoGen å¯ä»¥é…åˆ Prompt flow æ‹¼å‡ºä¸€ä¸ª XAgent

microsoft/autogen: Enable Next-Gen Large Language Model Applications. Join our Discord: <https://discord.gg/pAbnFJrkgZ>

<https://github.com/microsoft/autogen>

çœ‹ NVIDIA å‘äº†ç ”ç©¶ Blog è¯´è‡ªå·±ç”¨ç±»ä¼¼äº XAgent å¤–å¾ªç¯ + å†…å¾ªç¯çš„æ–¹å¼å»è®©å°æ¨¡å‹å’Œæ•°å­—å­ªç”Ÿèƒ½å¤Ÿå¯¹ã€Œå¯¹ç”¨äººæ‰‹è¿›è¡Œè½¬ç¬”è¿™æ ·çš„åŠ¨ä½œè¿›è¡Œå»ºæ¨¡ã€è¿™æ ·çš„å¤æ‚ä»»åŠ¡è¿›è¡Œå¾®è°ƒå’Œç›‘ç£ï¼Œå®ç°æ›´å…¨é¢å’Œæ™ºèƒ½çš„æ— ç›‘ç£å­¦ä¹ ã€‚

<https://blogs.nvidia.com/blog/2023/10/20/eureka-robotics-research/>

åœ¨ Minecraft ä¸­ç©æ¸¸æˆ

MineDojo/Voyager: An Open-Ended Embodied Agent with Large Language Models
<https://github.com/MineDojo/Voyager>

[arXiv [2305.16291] Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)

Voyager | An Open-Ended Embodied Agent with Large Language Models
<https://voyager.minedojo.org/>

[æ–°å‹äººå·¥æ™ºèƒ½ç®—æ³•å¯åœ¨5ç§’é’Ÿå†…ä»2Då›¾åƒä¸­åˆ›å»º3Dæ¨¡å‹ - VR / AR / 3D / IMAX - cnBeta.COM](https://www.cnbeta.com.tw/articles/tech/1396051.htm)

Frameworks for Serving LLMs. A comprehensive guide into LLMs inference and serving | by Sergei Savvov | Jul, 2023 | Medium | Better Programming
https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407

> X ä¸Šçš„ finï¼šâ€œè¿™æ˜¯ä¸€ç¯‡æ‰“ç ´GPTâ€œæ¶Œç°â€æ¦‚å¿µç¥è¯çš„è®ºæ–‡ï¼Œç»ˆäºè¯´å‡ºäº†æˆ‘ä¸€ç›´ä»¥æ¥çš„ä¸€ä¸ªç›´è§‰ï¼Œè¿™æ‰æ˜¯æ¯”è¾ƒç¬¦åˆäº‹ç‰©å‘å±•è§„å¾‹çš„ ä¸€å¥è¯æ€»ç»“ï¼Œæ‰€è°“GPTâ€œæ¶Œç°â€èƒ½åŠ›ï¼Œæ˜¯å› ä¸ºäººä¸ºä¿®æ”¹äº†â€œè¾¾æ ‡â€çš„è¯„ä»·æ ‡å‡†ï¼Œç»™äºº"æ¶Œç°"çš„é”™è§‰ ä¸€æ—¦ä½¿ç”¨æ›´åˆç†çš„è¯„ä»·æŒ‡æ ‡ï¼Œå°±ä¼šå‘ç°GPTèƒ½åŠ›å€¼éšç€æ¨¡å‹å¢å¤§æ˜¯çº¿æ€§å¢é•¿çš„ï¼Œä»è¯„ä»·æŒ‡æ ‡ä¸Šç›´æ¥è§£æ„äº†â€œæ¶Œç°â€â€¦ https://t.co/NJv7jCjM4hâ€ / X
https://twitter.com/fi56622380/status/1654386086746132481

Natureï¼šDeepMindå¤§æ¨¡å‹çªç ´60å¹´æ•°å­¦éš¾é¢˜ è§£æ³•è¶…å‡ºäººç±»å·²æœ‰è®¤çŸ¥ - AI äººå·¥æ™ºèƒ½ - cnBeta.COM
https://www.cnbeta.com.tw/articles/tech/1404741.htm

## å‚è€ƒèµ„æ–™

[LLM Introduction: Learn Language Models](https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d)

[How ChatGPT works: a deep dive | Dan Hollick](https://typefully.com/DanHollick/how-chatgpt-works-a-deep-dive-yA3ppZC)

[Greg Brockman: The inside story of ChatGPT's astonishing potential | TED Talk](https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential)

[What Are Transformer Models and How Do They Work?](https://txt.cohere.com/what-are-transformer-models/)

[é¢å‘å®Œå…¨å¤–è¡Œçš„chatGPTå’Œå¤§è¯­è¨€æ¨¡å‹çš„ä»‹ç» â€“ From nothing](http://hemin.live/archives/1143)

[Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models?utm_source=direct&utm_campaign=post&utm_medium=web)

[å…³äº AI çš„æ·±åº¦ç ”ç©¶ï¼šChatGPT æ­£åœ¨äº§ç”Ÿå¿ƒæ™ºå—ï¼Ÿ](https://www.bilibili.com/video/BV1uu4y1m7ak)

[ã€æ¸æ„ã€‘ä¸‡å­—ç§‘æ™®GPT4ä¸ºä½•ä¼šé¢ è¦†ç°æœ‰å·¥ä½œæµï¼›ä¸ºä½•ä½ è¦å…³æ³¨å¾®è½¯Copilotã€æ–‡å¿ƒä¸€è¨€ç­‰å¤§æ¨¡å‹ - å“”å“©å“”å“© bilibili](https://www.bilibili.com/video/BV1MY4y1R7EN/)

[2023å¹´3æœˆï¼Œäººç±»ç»ˆç©¶èµ°ä¸Šäº†ä¸€æ¡æ— æ³•å›å¤´çš„è·¯ - å“”å“©å“”å“© bilibili](https://www.bilibili.com/video/BV1VL411U7MU/)

[å¼ºçƒˆæ¨èï¼å°å¤§æå®æ¯…è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒTransformerè¯¦è§£ï¼- å“”å“©å“”å“© bilibili](https://www.bilibili.com/video/BV1v3411r78R)

## å»¶ä¼¸é˜…è¯»

- [when trees fall... | The New XOR Problem](https://blog.wtf.sg/posts/2023-02-03-the-new-xor-problem/)
- [hackerllama - The Random Transformer](https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/)
- [The Illustrated Transformer â€“ Jay Alammar â€“ Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/)
- [Solving Transformer by Hand: A Step-by-Step Math Example | by Fareed Khan | Dec, 2023 | Level Up Coding](https://levelup.gitconnected.com/understanding-transformers-from-start-to-end-a-step-by-step-math-example-16d4e64e6eb1)
- [Normcore LLM Reads](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)
