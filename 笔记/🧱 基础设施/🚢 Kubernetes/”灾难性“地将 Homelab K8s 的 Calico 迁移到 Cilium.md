---
tags:
  - å¼€å‘/äº‘åŸç”Ÿ
  - è®¡ç®—æœº/è®¡ç®—æœºç§‘å­¦/CS/è™šæ‹ŸåŒ–
  - å¼€å‘/äº‘åŸç”Ÿ/Kubernetes
  - å¼€å‘/å®¹å™¨åŒ–
  - å¼€å‘/è™šæ‹ŸåŒ–
  - å¼€å‘/å®¹å™¨åŒ–/Docker
  - å¼€å‘/äº‘åŸç”Ÿ/Docker
  - å‘½ä»¤è¡Œ/kubectl
  - å‘½ä»¤è¡Œ/kubeadm
  - å‘½ä»¤è¡Œ/containerd
  - å‘½ä»¤è¡Œ/docker
  - è½¯ä»¶/äº‘åŸç”Ÿ/kubeadm
  - è½¯ä»¶/äº‘åŸç”Ÿ/kubelet
  - è½¯ä»¶/äº‘åŸç”Ÿ/kubectl
  - è½¯ä»¶/äº‘åŸç”Ÿ/containerd
  - è½¯ä»¶/äº‘åŸç”Ÿ/docker
  - å¼€å‘/è™šæ‹ŸåŒ–/cgroup
  - è¿ç»´/äº‘åŸç”Ÿ/Kubernetes/K8s
  - å¼€å‘/äº‘åŸç”Ÿ/Kubernetes/K8s
  - è¿ç»´/äº‘åŸç”Ÿ/Kubernetes
  - è®¡ç®—æœº/ç½‘ç»œ
  - è¿ç»´/Cilium
  - å‘½ä»¤è¡Œ/cilium
  - è½¯ä»¶/äº‘åŸç”Ÿ/Cilium
  - è½¯ä»¶/äº‘åŸç”Ÿ/kube-proxy
  - è®¡ç®—æœº/ç½‘ç»œ/Calico
  - è¿ç»´/Calico
  - è½¯ä»¶/äº‘åŸç”Ÿ/Calico
  - è®¡ç®—æœº/ç½‘ç»œ/Cilium
  - å¼€å‘/æ ‡è®°è¯­è¨€/YAML
---
# â€ç¾éš¾æ€§â€œåœ°å°† Homelab K8s çš„ Calico è¿ç§»åˆ° Cilium

### æ–‡æ¡£å…¼å®¹æ€§

| ä¸»ä½“         | ç‰ˆæœ¬å· | æ–‡æ¡£åœ°å€ï¼ˆå¦‚æœæœ‰ï¼‰                |
| ------------ | ------ | --------------------------------- |
| Debian       | 11     |                                   |
| Kubernetes   | 1.28   | <https://v1-28.docs.kubernetes.io/> |
| Docker       | 24.0.2 | <https://docs.docker.com/>          |
| containerd   | 1.7.6  |                                   |
| Linux kernel | 5.10.0 |                                   |
| Calico       |        |                                   |
| Cilium       | 1.14.2 | <https://docs.cilium.io/en/v1.14/>  |
| helm         | v3.9.0 | <https://helm.sh/docs/>             |

## ç®€å•åˆ é™¤ Calico ä¹‹åå®‰è£… Cilium

### å®‰è£… Cilium

æ ¹æ® [Cilium Quick Installation](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/) çš„è¦æ±‚è¿è¡Œï¼š

```shell
sudo cilium install --version 1.14.2
```

æ¥å®‰è£… Ciliumã€‚

ç­‰å¾…ä¸€æ®µæ—¶é—´ä¹‹åå‘ç°æˆ‘ä»¬çš„ KubeSphere é¢æ¿ä¸å†å“åº”äº†ï¼Œè€Œæ˜¯è¿”å›äº† 502 é”™è¯¯ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ KubeSphere çš„ API æœåŠ¡å™¨ï¼š

```shell
$ sudo kubectl describe pods -n kubesphere-system ks-apiserver-6cd95fb98f-kvdkb

...

Events:
  Type     Reason                  Age                From     Message
  ----     ------                  ----               ----     -------
  Warning  Unhealthy               22m (x8 over 23m)  kubelet  Liveness probe failed: Get "http://10.233.90.228:9090/kapis/version": dial tcp 10.233.90.228:9090: connect: invalid argument
  Normal   Killing                 22m                kubelet  Container ks-apiserver failed liveness probe, will be restarted
  Normal   Pulled                  22m                kubelet  Container image "kubesphere/ks-apiserver:v3.3.1" already present on machine
  Normal   Created                 22m                kubelet  Created container ks-apiserver
  Normal   Started                 22m                kubelet  Started container ks-apiserver
  Warning  FailedCreatePodSandBox  20m                kubelet  Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "ed593ddb9cd630d2fc136e83c31d464cb97fa70712b2f4f85f0f5f73b38f055b" network for pod "ks-apiserver-6cd95fb98f-kvdkb": networkPlugin cni failed to set up pod "ks-apiserver-6cd95fb98f-kvdkb_kubesphere-system" network: unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 seconds timeout: Get "http://localhost/v1/config": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory
Is the agent running?
  Warning  FailedCreatePodSandBox  18m  kubelet  Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "5001e5aa69fecab537abe27c5900d84a6049f50883695d04a9c9f1ed734a1b20" network for pod "ks-apiserver-6cd95fb98f-kvdkb": networkPlugin cni failed to set up pod "ks-apiserver-6cd95fb98f-kvdkb_kubesphere-system" network: unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 seconds timeout: Get "http://localhost/v1/config": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory
Is the agent running?
```

å¯ä»¥å‘ç° Cilium å…¶å®æ²¡æœ‰æ­£å¸¸è¿è¡Œèµ·æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ `kubelet` æ€ä¹ˆæ ·ï¼š

```shell
$ sudo systemctl status kubelet

â— kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf
     Active: active (running) since Sat 2023-09-30 20:10:16 CST; 34min ago
       Docs: http://kubernetes.io/docs/
   Main PID: 506 (kubelet)
      Tasks: 175 (limit: 9830)
     Memory: 325.8M
        CPU: 4min 55.659s
     CGroup: /system.slice/kubelet.service
             â”œâ”€  506 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=s>
             â”œâ”€50849 /opt/cni/bin/cilium-cni
             â”œâ”€50935 /opt/cni/bin/cilium-cni
             â”œâ”€50971 /opt/cni/bin/cilium-cni
             â”œâ”€50972 /opt/cni/bin/cilium-cni
             â”œâ”€51015 /opt/cni/bin/cilium-cni
             â”œâ”€51283 /opt/cni/bin/cilium-cni
             â”œâ”€51306 /opt/cni/bin/cilium-cni
             â”œâ”€51323 /opt/cni/bin/cilium-cni
             â”œâ”€51337 /opt/cni/bin/cilium-cni
             â”œâ”€51381 /opt/cni/bin/cilium-cni
             â”œâ”€51398 /opt/cni/bin/cilium-cni
             â””â”€51414 /opt/cni/bin/cilium-cni

9æœˆ 30 20:44:17 node1 kubelet[506]: E0930 20:44:17.246138     506 cni.go:361] "Error adding pod to network" err="unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 seconds timeout: Get \"http://localhost/v1/config\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?" pod="kubesphere-monitoring-system/kube-state-metrics-687d66b747-5s6rt"
9æœˆ 30 20:44:17 node1 kubelet[506]: E0930 20:44:17.614511     506 kubelet_volumes.go:245] "There were many similar errors. Turn up verbosity to see them." err="orphaned pod \"1f1a54f9-7023-4995-a4c0-de78>
9æœˆ 30 20:44:17 node1 kubelet[506]: E0930 20:44:17.986337     506 cni.go:361] "Error adding pod to network" err="unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 s>
9æœˆ 30 20:44:18 node1 kubelet[506]: E0930 20:44:18.428698     506 cni.go:361] "Error adding pod to network" err="unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 seconds timeout: Get \"http://localhost/v1/config\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?" pod="kubesphere-monitoring-system/kube-state-metrics-687d66b747-5s6rt"
9æœˆ 30 20:44:19 node1 kubelet[506]: E0930 20:44:19.582819     506 kubelet_volumes.go:245] "There were many similar errors. Turn up verbosity to see them." err="orphaned pod \"1f1a54f9-7023-4995-a4c0-de78>
9æœˆ 30 20:44:20 node1 kubelet[506]: E0930 20:44:20.573676     506 remote_runtime.go:116] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox contain>
9æœˆ 30 20:44:20 node1 kubelet[506]: E0930 20:44:20.573860     506 kuberuntime_sandbox.go:70] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \">
9æœˆ 30 20:44:20 node1 kubelet[506]: E0930 20:44:20.573973     506 kuberuntime_manager.go:819] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \">
9æœˆ 30 20:44:20 node1 kubelet[506]: E0930 20:44:20.574210     506 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-state-metrics-687d66b747-5s6rt_kubespher>
9æœˆ 30 20:44:21 node1 kubelet[506]: E0930 20:44:21.601638     506 kubelet_volumes.go:245] "There were many similar errors. Turn up verbosity to see them." err="orphaned pod \"1f1a54f9-7023-4995-a4c0-de78>
```

å¯ä»¥å‘ç°ä¹Ÿæ˜¯ä¸€æ ·çš„æŠ¥é”™ï¼š

```shell
9æœˆ 30 20:44:18 node1 kubelet[506]: E0930 20:44:18.428698     506 cni.go:361] "Error adding pod to network" err="unable to connect to Cilium daemon: failed to create cilium agent client after 30.000000 seconds timeout: Get \"http://localhost/v1/config\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?" pod="kubesphere-monitoring-system/kube-state-metrics-687d66b747-5s6rt"
```

è¿™è¯´æ˜æˆ‘ä»¬çš„ Cilium æ— æ³•è¿è¡Œèµ·æ¥ï¼Œä¹Ÿè®¸æ˜¯ Calico ç›¸å…³çš„èµ„æºæ²¡æœ‰æ¸…ç†å¹²å‡€ã€‚

## ç§»é™¤å†²çªçš„ï¼Œå°šæœªå®Œå…¨æ¸…ç†å¹²å‡€çš„ Calico ç›¸å…³çš„èµ„æº

å¯ä»¥æ ¹æ®æ–‡æ¡£[[å®Œå…¨å¸è½½ä½¿ç”¨ KubeKey å®‰è£…çš„ Calico]] ä¸­å»ºè®®çš„æ–¹å¼å’Œæ­¥éª¤æ¥è¿›è¡Œåˆ é™¤ã€‚

ç„¶åæˆ‘ä»¬éœ€è¦é‡æ–°å®‰è£…ä¸€ä¸‹ Ciliumã€‚

## å¸è½½ Cilium

æˆ‘ä»¬å†è¿è¡Œä¸€ä¸‹ `cilium uninstall` è¯•è¯•çœ‹ï¼Œç„¶ååˆ é™¤ Cilium çš„ CNI é…ç½®ï¼š

```shell
sudo rm -rf /etc/cni/net.d/05-cilium.conflist
sudo rm -rf /etc/cni/net.d/10-calico.conflist.cilium_bak
```

ç§»é™¤ä¹‹åå‘ç°å®¹å™¨è¿˜æ˜¯åœ¨æŠ¥é”™ï¼Œæˆ‘ä»¬ç”¨ `kubectl describe` çœ‹çœ‹é…ç½®å’Œäº‹ä»¶ï¼š

```shell
sudo kubectl describe pod redis-v1-fb59985cc-lj4j2 -n bots

Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  45m    default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.cilium.io/agent-not-ready: }, that the pod didn't tolerate.
  Warning  FailedScheduling  44m    default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.cilium.io/agent-not-ready: }, that the pod didn't tolerate.
```

å¯ä»¥å‘ç°æœ‰ä¸¤ä¸ªæ±¡ç‚¹å’Œå…³äº `node.cilium.io/agent-not-ready` æ±¡ç‚¹çš„æŠ¥é”™ï¼Œä½†æ˜¯æˆ‘ä»¬å·²ç»æŠŠ Cilium ç§»é™¤äº†ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ±¡ç‚¹æˆ‘ä»¬ä¸æ¸…æ¥šæ˜¯å“ªé‡Œé…ç½®ä¸Šæ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ï¼š

```shell
sudo kubectl taint nodes node1 node.cilium.io/agent-not-ready-
sudo kubectl taint nodes node2 node.cilium.io/agent-not-ready-
sudo kubectl taint nodes node3 node.cilium.io/agent-not-ready-
```

ç»™ç§»é™¤äº†ï¼Œç§»é™¤ä¹‹åå†æ¬¡é‡å¯å…¨éƒ¨èŠ‚ç‚¹çš„ `kubelet` è¯•è¯•çœ‹æ˜¯å¦è¿˜æœ‰æ®‹ä½™ï¼š

```shell
sudo systemctl restart kubelet
```

è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å†æ¬¡è§‚å¯Ÿå°±èƒ½å‘ç°åªå‰©ä¸‹æˆ‘ä»¬é¢„æœŸå†…çš„ `node.kubernetes.io/not-ready`ï¼ˆK8s èŠ‚ç‚¹æœªå‡†å¤‡å¥½ï¼‰æ±¡ç‚¹äº†ï¼š

```shell
$ sudo kubectl describe pods redis-v1-fb59985cc-p2r5r -n bots

...

Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  46m    default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.cilium.io/agent-not-ready: }, that the pod didn't tolerate.
  Warning  FailedScheduling  44m    default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.cilium.io/agent-not-ready: }, that the pod didn't tolerate.
  Warning  FailedScheduling  5m51s  default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.
```

è¿™ä¸ªæ—¶å€™ç”±äºæˆ‘ä»¬åœ¨ä¹‹å‰å·²ç»åˆ é™¤äº† Calicoï¼Œä¹Ÿåˆ é™¤äº†æœ‰å†²çªé…ç½®çš„ Ciliumï¼Œè¿™ä¸ªæ—¶å€™é¢„æœŸæƒ…å†µä¸‹æˆ‘ä»¬ä¼šå‡ºç° CNI æ’ä»¶ç¼ºå¤±çš„æŠ¥é”™ï¼š

```shell
$ sudo systemctl status kubelet

â— kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf
     Active: active (running) since Sat 2023-09-30 21:48:45 CST; 5min ago
       Docs: http://kubernetes.io/docs/
   Main PID: 23191 (kubelet)
      Tasks: 19 (limit: 9830)
     Memory: 52.4M
        CPU: 47.656s
     CGroup: /system.slice/kubelet.service
             â””â”€23191 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=s>

9æœˆ 30 21:53:55 node1 kubelet[23191]: E0930 21:53:55.698482   23191 pod_workers.go:951] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kubesphere-monitoring-system/kube-state-metrics-687d66b747-5s6rt" podUID=ba43f196-a8c9-4d87-a18c-e847cef665d0
```

## å®‰è£… Cilium

è¿™æ˜¯æ­£å¸¸çš„ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å†æ¬¡æ‰§è¡Œä¸€ä¸‹ Cilium å®˜æ–¹æ–‡æ¡£æ•™çš„å®‰è£…ï¼š

```shell
$ sudo cilium install --version 1.14.2

â„¹ï¸  Using Cilium version 1.14.2
ğŸ”® Auto-detected cluster name: cluster.local
ğŸ”® Auto-detected kube-proxy has been installed
```

ç­‰å¾…ä¸€ä¼šå„¿åœ¨æ‰§è¡Œï¼š

```shell
sudo cilium status --wait
```

ç„¶åå°±èƒ½çœ‹åˆ°æˆ‘ä»¬çš„ cilium ç°åœ¨çŠ¶æ€æ­£å¸¸äº†ï¼š

```shell
$ sudo cilium status --wait
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium-operator    Running: 1
                       cilium             Running: 3
Cluster Pods:          1/55 managed by Cilium
Helm chart version:    1.14.2
Image versions         cilium             quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 3
                       cilium-operator    quay.io/cilium/operator-generic:v1.14.2@sha256:52f70250dea22e506959439a7c4ea31b10fe8375db62f5c27ab746e3a2af866d: 1
```

## å‡ºç°ç½‘ç»œé—®é¢˜

### è”é€šæ€§æµ‹è¯•ä¸é€šè¿‡

å®‰è£…ä¹‹åæˆ‘ä»¬æ‰§è¡Œï¼š

```shell
sudo cilium connectivity test
```

æ¥è¿›è¡Œç½‘ç»œè¿æ¥æµ‹è¯•ï¼Œä¼šå‘ç° `cilium-test/client` éƒ¨ç½²å¤±è´¥äº†ï¼š

```shell
$ sudo cilium connectivity test

â„¹ï¸  Monitor aggregation detected, will skip some flow validation steps
âœ¨ [cluster.local] Creating namespace cilium-test for connectivity check...
âœ¨ [cluster.local] Deploying echo-same-node service...
âœ¨ [cluster.local] Deploying DNS test server configmap...
âœ¨ [cluster.local] Deploying same-node deployment...
âœ¨ [cluster.local] Deploying client deployment...
âœ¨ [cluster.local] Deploying client2 deployment...
âœ¨ [cluster.local] Deploying echo-other-node service...
âœ¨ [cluster.local] Deploying other-node deployment...
âœ¨ [host-netns] Deploying cluster.local daemonset...
âœ¨ [host-netns-non-cilium] Deploying cluster.local daemonset...
âœ¨ [cluster.local] Deploying echo-external-node deployment...
âŒ› [cluster.local] Waiting for deployment cilium-test/client to become ready...
connectivity test failed: timeout reached waiting for deployment cilium-test/client to become ready (last error: only 0 of 1 replicas are available)
```

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿä¸€ä¸‹äº‹ä»¶ï¼š

```shell
$ sudo kubectl -n cilium-test describe deployment client

...

Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set client-7dccdf9bdf to 1
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ `deployment` å¤±è´¥åœ¨æ‰©å®¹åˆ° 1 ä»½ Replicaï¼Œé‚£æˆ‘ä»¬çœ‹çœ‹ Pod è¿™è¾¹æ˜¯å¡åœ¨å“ªä¸ªç¯èŠ‚äº†ï¼š

```shell
$ sudo kubectl -n cilium-test get pods
NAME                                  READY   STATUS              RESTARTS   AGE
client-7dccdf9bdf-tmc5h               0/1     ImagePullBackOff    0          20m
client2-5fd767c97f-bl84c              0/1     ImagePullBackOff    0          20m
...
```

### é•œåƒæ‹‰å–å¤±è´¥

çœ‹èµ·æ¥æ˜¯ `ImagePullBackoff` äº†ï¼Œæ— æ³•æ‹‰å–é•œåƒï¼Œæˆ‘ä»¬çœ‹çœ‹å…·ä½“çš„äº‹ä»¶å§ï¼š

```shell
$ sudo kubectl describe pods -n cilium-test client-7dccdf9bdf-tmc5h

...

Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  20m                  default-scheduler  Successfully assigned cilium-test/client-7dccdf9bdf-tmc5h to node1
  Warning  Failed     6m41s                kubelet            Failed to pull image "quay.io/cilium/alpine-curl:v1.7.0@sha256:ccd0ed9da1752bab88a807647ad3cec65d460d281ab88988b60d70148783e751": rpc error: code = Unknown desc = Error response from daemon: Get "https://quay.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Failed     6m41s                kubelet            Error: ErrImagePull
  Normal   BackOff    6m40s                kubelet            Back-off pulling image "quay.io/cilium/alpine-curl:v1.7.0@sha256:ccd0ed9da1752bab88a807647ad3cec65d460d281ab88988b60d70148783e751"
  Warning  Failed     6m40s                kubelet            Error: ImagePullBackOff
  Normal   Pulling    6m27s (x2 over 19m)  kubelet            Pulling image "quay.io/cilium/alpine-curl:v1.7.0@sha256:ccd0ed9da1752bab88a807647ad3cec65d460d281ab88988b60d70148783e751"
```

çœ‹èµ·æ¥ä¹Ÿè®¸æ˜¯ Homelab è¿™è¾¹ç½‘ç»œæœ‰ç‚¹çˆ†ç‚¸ã€‚åœ¨è°ƒæ•´äº† Clash èŠ‚ç‚¹é€‰æ‹©ä¹‹åä¾ç„¶ä¸è¡Œï¼Œæˆ‘ä»¬å¾—æ·±å…¥è°ƒæŸ¥ä¸€ä¸‹ã€‚

### æ’æŸ¥ coredns å’Œç›¸å…³çš„ Pod

```shell
$ sudo kubectl get pods -n kube-system

NAME                                           READY   STATUS             RESTARTS       AGE
cilium-mdtzl                                   1/1     Running            0              8h
cilium-mmh9j                                   1/1     Running            0              8h
cilium-operator-8bf9464bf-xdb7l                1/1     Running            1 (8h ago)     8h
cilium-rqhvs                                   1/1     Running            0              8h
coredns-b5648d655-d8ltm                        0/1     ImagePullBackOff   0              11h
coredns-b5648d655-p5j5d                        0/1     ImagePullBackOff   0              8h
hubble-relay-85cff75759-hlpk4                  0/1     ErrImagePull       0              7h19m
hubble-ui-78b9fbc9cb-hqmw6                     0/2     ErrImagePull       0              7h19m
kube-apiserver-node1                           1/1     Running            19 (9h ago)    308d
kube-controller-manager-node1                  1/1     Running            30 (8h ago)    308d
kube-proxy-bjrg4                               1/1     Running            19 (9h ago)    308d
kube-proxy-h6cqw                               1/1     Running            17 (9h ago)    308d
kube-proxy-s9pbw                               1/1     Running            16 (9h ago)    308d
kube-scheduler-node1                           1/1     Running            28 (8h ago)    308d
nodelocaldns-56lvx                             1/1     Running            16 (9h ago)    308d
nodelocaldns-h5hf4                             1/1     Running            17 (9h ago)    308d
nodelocaldns-rk2fh                             1/1     Running            19 (9h ago)    308d
openebs-localpv-provisioner-57bbf864d5-2hkzr   0/1     ErrImagePull       15 (10h ago)   308d
snapshot-controller-0                          1/1     Running            15 (10h ago)   308d
```

çœ‹èµ·æ¥ `coredns` è‡ªå·±å°±èµ·ä¸æ¥äº†ã€‚æ·±å…¥æŸ¥çœ‹ä¸€ä¸‹ `coredns` è¿™ä¸ª Pod çš„ `ImagePullBackOff` çš„åŸå› ï¼š

```
$ sudo kubectl describe pods coredns-b5648d655-d8ltm -n kube-system

...

Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason   Age                    From     Message
  ----     ------   ----                   ----     -------
  Warning  Failed   18m (x77 over 8h)      kubelet  Failed to pull image "coredns/coredns:1.8.0": rpc error: code = Unknown desc = Error response from daemon: Get "https://registry-1.docker.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Normal   Pulling  13m (x86 over 8h)      kubelet  Pulling image "coredns/coredns:1.8.0"
  Normal   BackOff  3m28s (x1857 over 8h)  kubelet  Back-off pulling image "coredns/coredns:1.8.0"
```

### å®šä½åˆ°ç½‘ç»œé—®é¢˜

çœ‹èµ·æ¥ä¸å…‰æ˜¯ `quay.io` çš„é•œåƒæœåŠ¡æ— æ³•è®¿é—®åˆ°ï¼Œ`docker.io` ä¹Ÿä¸è¡Œã€‚æˆ‘ä»¬å¯ä»¥ç”¨ `curl` ç›´æ¥è¯•è¯•çœ‹ï¼š

```shell
$ curl https://registry-1.docker.io/v2/ -I
curl: (6) Could not resolve host: registry-1.docker.io
```

å‘ç°æ— æ³•è§£æäº†ï¼Œé‚£æˆ‘ä»¬çœ‹çœ‹æœ¬æœºçš„ DNS å‘¢ï¼š

```shell
$ dig registry-1.docker.io @8.8.8.8

; <<>> DiG 9.16.37-Debian <<>> registry-1.docker.io @8.8.8.8
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 19821
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;registry-1.docker.io.		IN	A

;; ANSWER SECTION:
registry-1.docker.io.	1	IN	A	18.215.138.58
registry-1.docker.io.	1	IN	A	52.1.184.176
registry-1.docker.io.	1	IN	A	34.194.164.123

;; Query time: 3 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Sun Oct 01 09:13:02 CST 2023
;; MSG SIZE  rcvd: 86
```

æ­£å¸¸ï¼Œä½†æ˜¯ `10.0.0.1` çš„ Homelab Gateway è·¯ç”±å™¨çš„ DNS è¿æ¥ä¸ä¸Šäº†ï¼š

```shell
$ dig registry-1.docker.io @10.0.0.1

; <<>> DiG 9.16.37-Debian <<>> registry-1.docker.io @10.0.0.1
;; global options: +cmd
;; connection timed out; no servers could be reached
```

```shell
$ ping 10.0.0.1

PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data.
64 bytes from 10.0.0.1: icmp_seq=1 ttl=63 time=0.695 ms
64 bytes from 10.0.0.1: icmp_seq=2 ttl=63 time=0.777 ms
64 bytes from 10.0.0.1: icmp_seq=3 ttl=63 time=0.536 ms
64 bytes from 10.0.0.1: icmp_seq=4 ttl=63 time=0.754 ms
^C
--- 10.0.0.1 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 5998ms
rtt min/avg/max/mdev = 0.536/0.685/0.777/0.078 ms
```

èƒ½ ping é€šï¼Œä½†æ˜¯å¦‚æœç”¨ mtr çœ‹çš„è¯èƒ½å‘ç° 10.0.0.1 çš„ hostname ä¸æ˜¯æˆ‘ä»¬çš„ Gateway è·¯ç”±å™¨ä¼šæ­£å¸¸è¿”å›çš„ hostnameï¼š

```shell
node1 (10.0.2.208) -> 10.0.0.1                                                                                                                                                     2023-10-01T09:18:40+0800
Keys:  Help   Display mode   Restart statistics   Order of fields   quit
                                                                                                                                                                   Packets               Pings
 Host                                                                                                                                                            Loss%   Snt   Last   Avg  Best  Wrst StDev
 1. (waiting for reply)
 2. 10.0.0.1                                                                                                                                                      0.0%     4    0.8   0.8   0.7   0.9   0.0
```

åº”è¯¥æ˜¯è·¯ç”±åˆ°äº†åˆ«çš„åœ°æ–¹å»äº†ï¼Œå¯ä»¥å†æ£€æŸ¥ä¸€ä¸‹è·¯ç”±è¡¨ï¼š

```shell
$ ip route
default via 10.0.0.1 dev eth0
10.0.0.0/24 via 10.0.2.208 dev cilium_host proto kernel src 10.0.2.208 mtu 1450
10.0.0.0/16 dev eth0 proto kernel scope link src 10.0.0.124
10.0.1.0/24 via 10.0.2.208 dev cilium_host proto kernel src 10.0.2.208 mtu 1450
10.0.2.0/24 via 10.0.2.208 dev cilium_host proto kernel src 10.0.2.208
10.0.2.208 dev cilium_host proto kernel scope link
10.24.0.0/24 dev eth1 proto kernel scope link src 10.24.0.2
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
```

ç¡®å®æ˜¯è¿™æ ·ã€‚

æˆ‘ä»¬é‡æ–°ç¿»é˜…æ–‡æ¡£ [Cluster Scope (Default) â€” Cilium 1.14.2 documentation](https://docs.cilium.io/en/stable/network/concepts/ipam/cluster-pool/#ipam-crd-cluster-pool): å¯ä»¥å‘ç°æ–‡æ¡£ä¸­æè¿°è¯´ï¼š

> `10.0.0.0/8`Â æ˜¯é»˜è®¤çš„ Pod CIDRã€‚å¦‚æœæ‚¨çš„èŠ‚ç‚¹ç½‘ç»œä½äºåŒä¸€èŒƒå›´å†…ï¼Œæ‚¨å°†å¤±å»ä¸å…¶ä»–èŠ‚ç‚¹çš„è¿æ¥ã€‚å‡å®šæ‰€æœ‰å‡ºå£æµé‡éƒ½é’ˆå¯¹ç»™å®šèŠ‚ç‚¹ä¸Šçš„ Podï¼Œè€Œä¸æ˜¯å…¶ä»–èŠ‚ç‚¹ã€‚
>
> æ‚¨å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼è§£å†³ï¼š
>
> - æ˜¾å¼å°†Â `clusterPoolIPv4PodCIDRList`Â è®¾ç½®ä¸ºä¸å†²çªçš„ CIDR
> - ä¸ºæ‚¨çš„èŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„ CIDR

OKï¼Œé‚£æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯çœŸçš„æ˜¯è¿™æ ·ï¼š

```shell
$ sudo kubectl get ciliumnodes

NAME    CILIUMINTERNALIP   INTERNALIP   AGE
node1   10.0.2.208         10.24.0.2    14h
node2   10.0.1.157         10.24.0.4    14h
node3   10.0.0.78          10.24.0.5    14h
```

ä¸å¤ªå¯¹åŠ²ï¼Œæˆ‘ä»¬ describe çœ‹çœ‹ï¼š

```shell
$ sudo kubectl describe ciliumnodes
Name:         node1
Namespace:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/arch=amd64
              kubernetes.io/hostname=node1
              kubernetes.io/os=linux
              node-access=ssh
              node-role.kubernetes.io/control-plane=
              node-role.kubernetes.io/master=
              node-role.kubernetes.io/worker=
              node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:  <none>
API Version:  cilium.io/v2
Kind:         CiliumNode

...

Spec:
  Addresses:
    Ip:    10.24.0.2
    Type:  InternalIP
    Ip:    10.0.2.208
    Type:  CiliumInternalIP
  Alibaba - Cloud:
  Azure:
  Encryption:
  Eni:
  Health:
    ipv4:  10.0.2.140
  Ingress:
  Ipam:
    Pod CID Rs:
      10.0.2.0/24
    Pools:
Status:
  Alibaba - Cloud:
  Azure:
  Eni:
  Ipam:
    Operator - Status:
Events:  <none>
```

```shell
Name:         node2
Namespace:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/arch=amd64
              kubernetes.io/hostname=node2
              kubernetes.io/os=linux
              node-access=ssh
              node-role.kubernetes.io/worker=
Annotations:  <none>
API Version:  cilium.io/v2
Kind:         CiliumNode

...

Spec:
  Addresses:
    Ip:    10.24.0.4
    Type:  InternalIP
    Ip:    10.0.1.157
    Type:  CiliumInternalIP
  Alibaba - Cloud:
  Azure:
  Encryption:
  Eni:
  Health:
    ipv4:  10.0.1.242
  Ingress:
  Ipam:
    Pod CID Rs:
      10.0.1.0/24
    Pools:
Status:
  Alibaba - Cloud:
  Azure:
  Eni:
  Ipam:
    Operator - Status:
Events:  <none>
```

```shell
Name:         node3
Namespace:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/arch=amd64
              kubernetes.io/hostname=node3
              kubernetes.io/os=linux
              node-access=ssh
              node-role.kubernetes.io/worker=
Annotations:  <none>
API Version:  cilium.io/v2
Kind:         CiliumNode

...

Spec:
  Addresses:
    Ip:    10.24.0.5
    Type:  InternalIP
    Ip:    10.0.0.78
    Type:  CiliumInternalIP
  Alibaba - Cloud:
  Azure:
  Encryption:
  Eni:
  Health:
    ipv4:  10.0.0.87
  Ingress:
  Ipam:
    Pod CID Rs:
      10.0.0.0/24
    Pools:
Status:
  Alibaba - Cloud:
  Azure:
  Eni:
  Ipam:
    Operator - Status:
Events:  <none>
```

æ‰€ä»¥æ ¹æ®æ–‡æ¡£ [Migrating a cluster to Cilium â€” Cilium 1.14.2 documentation](https://docs.cilium.io/en/stable/installation/k8s-install-migration/) çš„è¯´æ˜ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åœ¨å®‰è£…çš„æ—¶å€™é¢å¤–é…ç½®ä¸€ä¸‹ CIDR æ‰èƒ½è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

## ä¸ºä¿®å¤ CIDR å†²çªé‡è£… Cilium

#### å¸è½½ Cilium

é‚£æˆ‘ä»¬ç°åœ¨å…ˆæ ¹æ®[[å®Œå…¨å¸è½½ä½¿ç”¨ Helm å®‰è£…çš„ Cilium]] æ–‡æ¡£çš„æŒ‡å¼•å®Œå…¨åˆ é™¤ cilium ç„¶åå†è¯•ä¸€æ¬¡ã€‚

#### æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ¢å¤

æ¸…ç†ä¹‹åæˆ‘ä»¬å†æ¬¡æ£€æŸ¥ç½‘ç»œè”é€šæ€§ï¼š

```shell
$ ping baidu.com
PING baidu.com (110.242.68.66) 56(84) bytes of data.
64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=1 ttl=62 time=0.564 ms
64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=2 ttl=62 time=0.569 ms
64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=3 ttl=62 time=0.514 ms
64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=4 ttl=62 time=0.874 ms
^C
--- baidu.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2998ms
rtt min/avg/max/mdev = 0.514/0.630/0.874/0.142 ms
```

```shell
$ curl baidu.com -L -I
HTTP/1.1 200 OK
Date: Fri, 06 Oct 2023 03:43:26 GMT
Server: Apache
Last-Modified: Tue, 12 Jan 2010 13:48:00 GMT
ETag: "51-47cf7e6ee8400"
Accept-Ranges: bytes
Content-Length: 81
Cache-Control: max-age=86400
Expires: Sat, 07 Oct 2023 03:43:26 GMT
Connection: Keep-Alive
Content-Type: text/html
```

```shell
$ ping google.com
PING google.com (142.250.204.46) 56(84) bytes of data.
64 bytes from hkg07s38-in-f14.1e100.net (142.250.204.46): icmp_seq=1 ttl=62 time=0.660 ms
64 bytes from hkg07s38-in-f14.1e100.net (142.250.204.46): icmp_seq=2 ttl=62 time=0.570 ms
64 bytes from hkg07s38-in-f14.1e100.net (142.250.204.46): icmp_seq=3 ttl=62 time=0.611 ms
64 bytes from hkg07s38-in-f14.1e100.net (142.250.204.46): icmp_seq=4 ttl=62 time=0.609 ms
^C
--- google.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3002ms
rtt min/avg/max/mdev = 0.570/0.612/0.660/0.031 ms
```

```shell
$ curl google.com -L -I
HTTP/1.1 301 Moved Permanently
Location: http://www.google.com/
Content-Type: text/html; charset=UTF-8
Content-Security-Policy-Report-Only: object-src 'none';base-uri 'self';script-src 'nonce-TZgO-fIhEQR2thcKEH04rw' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp
Date: Fri, 06 Oct 2023 03:43:46 GMT
Expires: Sun, 05 Nov 2023 03:43:46 GMT
Cache-Control: public, max-age=2592000
Server: gws
Content-Length: 219
X-XSS-Protection: 0
X-Frame-Options: SAMEORIGIN

HTTP/1.1 200 OK
Content-Type: text/html; charset=ISO-8859-1
Content-Security-Policy-Report-Only: object-src 'none';base-uri 'self';script-src 'nonce-Ln-1anLLMO6ubPnv34o45g' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp
P3P: CP="This is not a P3P policy! See g.co/p3phelp for more info."
Date: Fri, 06 Oct 2023 03:43:46 GMT
Server: gws
X-XSS-Protection: 0
X-Frame-Options: SAMEORIGIN
Transfer-Encoding: chunked
Expires: Fri, 06 Oct 2023 03:43:46 GMT
Cache-Control: private
```

å‘ç°æ­£å¸¸äº†ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å¼€å§‹é‡æ–°å®‰è£…ã€‚

#### å¡«å……é…ç½®

æ–°çš„å®‰è£…æµç¨‹éœ€è¦èµ° Helmï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬éœ€è¦ç»™ Cilium çš„ Helm éƒ¨ç½²å‚æ•°é…ç½®ä¸€äº›é¢å¤–ä¿¡æ¯æ¥é˜»æ­¢ Cilium ä½¿ç”¨ 10.0.0.0/8 ä½œä¸º CIDRï¼Œæ–°å»º `cilium-values-migration.yaml` æ–‡ä»¶ï¼š

```yaml
# é»˜è®¤æ‰“å¼€ Hubble çš„ Relay å’Œ UIï¼Œè¿™æ ·ä¹‹åæˆ‘ä»¬å°±ä¸ç”¨å†å•ç‹¬æ‰§è¡Œ cilium hubble enable æ¥å¯ç”¨äº†
hubble:
  relay:
    enabled: true
  ui:
    enabled: true


ipam:
  # å¯ä»¥é˜…è¯» https://docs.cilium.io/en/stable/network/concepts/ipam/kubernetes/ äº†è§£æ›´å¤š
  mode: 'kubernetes'
  operator:
    # æ­¤å¤„çš„å­—é¢é‡éœ€è¦å’Œæˆ‘ä»¬çš„ Kubernetes åœ¨åˆ›å»ºçš„æ—¶å€™æŒ‡å®šçš„ Pod CIDR ç›¸åŒ
    clusterPoolIPv4PodCIDRList: '10.244.0.0/16'

# å¦‚æœä¸Šé¢æŒ‡å®šäº† ipam.operator.clusterPoolIPv4PodCIDRList é‚£ä¹ˆè¿™é‡Œå°±ä¹Ÿå¾—é…ç½®æˆä¸€æ ·çš„å­—é¢é‡
ipv4NativeRoutingCIDR: '10.244.0.0/16'

# å¦‚æœ tunnel ä¿æŒæ‰“å¼€ï¼Œè€Œä¸”æˆ‘ä»¬å¸è½½äº† kube-proxyï¼Œé‚£ä¹ˆéœ€è¦å¼€å¯ä¸‹é¢çš„ä¸¤ä¸ªé€‰é¡¹
enableIPv4Masquerade: true
enableIPv6Masquerade: true
```

æ³¨æ„è¿™ä¸ªé…ç½®æ–‡ä»¶é‡Œé¢çš„ `kubeProxyReplacement` å­—æ®µé…ç½®ï¼Œæ ¹æ® Cilium çš„ [Kubernetes Without kube-proxy](https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#kubeproxy-free) æ–‡æ¡£ï¼š

> Cilium çš„ kube-proxy æ›¿æ¢ä¾èµ–äº socket-LB åŠŸèƒ½ï¼Œè¿™éœ€è¦ v4.19.57ã€v5.1.16ã€v5.2.0 æˆ–æ›´é«˜ç‰ˆæœ¬çš„ Linux å†…æ ¸ã€‚ Linux å†…æ ¸ v5.3 å’Œ v5.8 æ·»åŠ äº†å…¶ä»–åŠŸèƒ½ï¼ŒCilium å¯ä»¥ä½¿ç”¨è¿™äº›åŠŸèƒ½æ¥è¿›ä¸€æ­¥ä¼˜åŒ– kube-proxy æ›¿æ¢å®ç°ã€‚
> è¯·æ³¨æ„ï¼Œv5.0.y å†…æ ¸æ²¡æœ‰è¿è¡Œ kube-proxy æ›¿æ¢æ‰€éœ€çš„ä¿®å¤ç¨‹åºï¼Œå› ä¸ºæ­¤æ—¶ v5.0.y ç¨³å®šå†…æ ¸å·²ç»ˆæ­¢ç”Ÿå‘½ (EOL)ï¼Œå¹¶ä¸”ä¸å†åœ¨å†…æ ¸ä¸Šç»´æŠ¤.orgã€‚å¯¹äºå•ç‹¬çš„å‘è¡Œç‰ˆç»´æŠ¤çš„å†…æ ¸ï¼Œæƒ…å†µå¯èƒ½æœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œè¯·æ£€æŸ¥æ‚¨çš„å‘è¡Œç‰ˆã€‚

æ‰€ä»¥è¿˜è¯·ç¡®ä¿ `uname -a` ä¸­è¾“å‡ºçš„å†…æ ¸ç‰ˆæœ¬æ»¡è¶³éœ€æ±‚ï¼Œå¦‚æœä¸æ»¡è¶³ï¼Œå¯ä»¥æ³¨é‡Šæˆ–è€…åˆ æ‰æ­¤è¡Œã€‚

ç¡®è®¤æ— è¯¯ä¹‹åæˆ‘ä»¬ç”¨ä¸‹é¢çš„å‘½ä»¤è¾“å‡º Helm å®‰è£…ç”¨çš„é…ç½®æ–‡ä»¶ï¼š

```shell
sudo cilium install --version 1.14.2 --values cilium-values-migration.yaml --dry-run-helm-values > cilium-values-initial.yaml
```

å¯ä»¥é€šè¿‡ [[cat è¾“å‡ºæ–‡ä»¶]] å‘½ä»¤æ£€æŸ¥ä¸€ä¸‹ï¼š

```shell
$ cat cilium-values-initial.yaml

cluster:
  name: kubernetes
enableIPv4Masquerade: true
enableIPv6Masquerade: true
hubble:
  relay:
    enabled: true
  ui:
    enabled: true
ipam:
  mode: kubernetes
  operator:
    clusterPoolIPv4PodCIDRList: 10.244.0.0/16
ipv4NativeRoutingCIDR: 10.244.0.0/16
# æ³¨æ„ç¡®è®¤ä¸€ä¸‹ Kubernetes API Server çš„ IP æ˜¯å¦æ˜¯è¿™ä¸ª
k8sServiceHost: lb.kubesphere.local # [!code hl]
# æ³¨æ„ç¡®è®¤ä¸€ä¸‹ Kubernetes API Server çš„ç«¯å£æ˜¯å¦æ˜¯è¿™ä¸ª
k8sServicePort: 6443 # [!code hl]
kubeProxyReplacement: strict
operator:
  replicas: 1
serviceAccounts:
  cilium:
    name: cilium
  operator:
    name: cilium-operator
tunnel: vxlan
```

#### ç§»é™¤åŸæœ¬çš„ `kube-proxy`

> [!TIP] å¦‚æœä½ é€‰æ‹©ä½¿ç”¨ `kubeProxyReplacement` å‚æ•°æ›¿ä»£ `kube-proxy`
> å¯ä»¥è·Ÿéš[[å®Œå…¨å¸è½½é›†ç¾¤å†…çš„ `kube-proxy`]] æ–‡æ¡£çš„æŒ‡å¼•å¤‡ä»½å’Œåˆ é™¤ `kube-proxy` ç›¸å…³çš„é…ç½®å’Œèµ„æºã€‚

#### å®‰è£… Cilium

å¦‚æœä½ è¿˜æ²¡æœ‰æ·»åŠ  Cilium çš„ Repoï¼Œå¯ä»¥åŠ ä¸€ä¸‹ï¼š

```shell
sudo helm repo add cilium https://helm.cilium.io/
```

ç„¶åå®‰è£…ï¼š

```shell
sudo helm install cilium cilium/cilium --namespace kube-system --values cilium-values-initial.yaml
```

ç„¶åæˆ‘ä»¬å†ç­‰ç­‰çœ‹ï¼š

```shell
$ sudo cilium status --wait
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/Â¯Â¯\__/    Hubble Relay:       OK
    \__/       ClusterMesh:        disabled

Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
Deployment             hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment             hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
Containers:            hubble-ui          Running: 1
                       hubble-relay       Running: 1
                       cilium             Running: 3
                       cilium-operator    Running: 1
Cluster Pods:          10/10 managed by Cilium
Helm chart version:    1.14.2
Image versions         cilium-operator    quay.io/cilium/operator-generic:v1.14.2@sha256:52f70250dea22e506959439a7c4ea31b10fe8375db62f5c27ab746e3a2af866d: 1
                       hubble-ui          quay.io/cilium/hubble-ui:v0.12.0@sha256:1c876cfa1d5e35bc91e1025c9314f922041592a88b03313c22c1f97a5d2ba88f: 1
                       hubble-ui          quay.io/cilium/hubble-ui-backend:v0.12.0@sha256:8a79a1aad4fc9c2aa2b3e4379af0af872a89fcec9d99e117188190671c66fc2e: 1
                       hubble-relay       quay.io/cilium/hubble-relay:v1.14.2@sha256:a89030b31f333e8fb1c10d2473250399a1a537c27d022cd8becc1a65d1bef1d6: 1
                       cilium             quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 3
```

ç°åœ¨å†æ¥æµ‹è¯•çš„è¯å°±æ˜¯æ­£å¸¸çš„äº†ï¼š

```shell
$ sudo cilium connectivity test

...

âœ… All 42 tests (295 actions) successful, 13 tests skipped, 0 scenarios skipped.
```

## å‚è€ƒèµ„æ–™

- [k8sç³»åˆ—15-calicoæœ‰æŸè¿ç§»è‡³cilium - TinyChen's Studio - äº’è”ç½‘æŠ€æœ¯å­¦ä¹ å·¥ä½œç»éªŒåˆ†äº«](https://tinychen.com/20230201-k8s-15-migrate-cni-from-calico-to-cilium/)
- [coredns [ERROR] plugin/errors: 2 read udp 10.244.235.249:55567->10.96.0.10:53: i/o timeout Â· Issue #86762 Â· kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/issues/86762)
- [When using cilium as Kubernetes network CNI, the coredns is running but not-ready, healthcheck failed and plugin/errors HINFO: read udp i/o timeout Â· Issue #111105 Â· kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/issues/111105)
- [ciliumåœ¨kubernetesä¸­çš„ç”Ÿäº§å®è·µäºŒ(ciliuméƒ¨ç½²) | Z.S.K.'s Records](https://izsk.me/2023/06/03/cilium-on-kubernetes-install/)
- [Tutorial: How to Migrate to Cilium (Part 1) - Isovalent](https://isovalent.com/blog/post/tutorial-migrating-to-cilium-part-1/)
- [Cilium - KubernetesæŒ‡å—](https://kubernetes.feisky.xyz/extension/network/cilium)
- [coredns [ERROR] plugin/errors: 2 read udp 10.244.235.249:55567->10.96.0.10:53: i/o timeout Â· Issue #86762 Â· kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/issues/86762)
- [DNS rules don't work anymore on kubernetes 1.18 with cilium 1.8 Â· Issue #13308 Â· cilium/cilium](https://github.com/cilium/cilium/issues/13308)
- [I set ipam.operator.clusterPoolIPv4PodCIDR=10.1.0.0/16 why the pod ip allocated is still 10.0 Â· Issue #23872 Â· cilium/cilium](https://github.com/cilium/cilium/issues/23872)
- [When using cilium as Kubernetes network CNI, the coredns is running but not-ready, healthcheck failed and plugin/errors HINFO: read udp i/o timeout Â· Issue #111105 Â· kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/issues/111105)
- [linux - Kubernetes Nodes are not reachable and cannot reach local network after installing cilium - Server Fault](https://serverfault.com/questions/1103034/kubernetes-nodes-are-not-reachable-and-cannot-reach-local-network-after-installi)
- [Creating a cluster with kubeadm | Kubernetes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network)
- [kubernetes - Container runtime network not ready: cni config uninitialized - Stack Overflow](https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized)
- [åˆ é™¤cilium ebpf/bpfç¨‹åº/å®Œå…¨å¸è½½cilium](https://www.jianshu.com/p/896ec00b9661)
- [æ— æ³•å½»åº•æ¸…ç†ciliumç•™ä¸‹çš„é—®é¢˜](https://gist.github.com/aliasmee/6c7e5fb433c8fd303b07f0081fc83677)
- [ä½¿ç”¨ Cilium æ›¿æ¢ Calico â€“ é™ˆå°‘æ–‡çš„ç½‘ç«™](https://www.chenshaowen.com/blog/how-to-use-cilium-to-replace-calico.html)
- [Troubleshooting Clusters | Kubernetes](https://kubernetes.io/docs/tasks/debug/debug-cluster/)
