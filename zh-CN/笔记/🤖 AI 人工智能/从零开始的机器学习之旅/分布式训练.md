---
tags:
  - AI
  - AI/æ•™ç¨‹
---

# åˆ†å¸ƒå¼è®­ç»ƒ

ç°åœ¨å·²ç»ä¸æ¨èä½¿ç”¨ `DataParallel` äº†[^1]ï¼Œ

`rank`

`gpu`

`local_rank`ï¼šè¿›ç¨‹é˜¶åºï¼ˆrankï¼‰

`global_rank`

`world_size`ï¼šæ€»è¿›ç¨‹æ•°

`torchrun`

`torch.distributed.launch`

`nproc_per_node`
## ä»€ä¹ˆæ˜¯ DDPï¼Ÿ

`DistributedDataParallel`

`NCCL`

`RDMA`

`NVML`

`torch.distributed`

åˆ†å¸ƒå¼è®¡ç®—

All-Reduce

Reduced ring

> In CPython, theÂ **global interpreter lock**, orÂ **GIL**, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. The GIL prevents race conditions and ensures thread safety. A nice explanation ofÂ [how the Python GIL helps in these areas can be found here](https://python.land/python-concurrency/the-python-gil). In short, this mutex is necessary mainly because CPython's memory management is not thread-safe.
>
> æ¥æºï¼š [GlobalInterpreterLock - Python Wiki](https://wiki.python.org/moin/GlobalInterpreterLock)

Distributed training with ğŸ¤— Accelerate
https://huggingface.co/docs/transformers/accelerate

[TorchX â€” PyTorch/TorchX main documentation](https://pytorch.org/torchx/latest/)

[pytorch/torchx: TorchX is a universal job launcher for PyTorch applications. TorchX is designed to have fast iteration time for training/research and support for E2E production ML pipelines when you're ready.](https://github.com/pytorch/torchx)

[TorchElastic Kubernetes â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/elastic/kubernetes.html)

```shell
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name     | Type    | Params
-------------------------------------
0 | conv1    | Conv2d  | 320
1 | conv2    | Conv2d  | 18.5 K
2 | dropout1 | Dropout | 0
3 | dropout2 | Dropout | 0
4 | fc1      | Linear  | 1.2 M
5 | fc2      | Linear  | 1.3 K
-------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.800     Total estimated model params size (MB)
```

```shell
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
```

```shell
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
```

Pytorch DDP

[Getting Started with Distributed Data Parallel â€” PyTorch Tutorials 2.2.0+cu121 documentation](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
[Writing Distributed Applications with PyTorch â€” PyTorch Tutorials 2.2.0+cu121 documentation](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
[Saving and Loading Models â€” PyTorch Tutorials 2.2.0+cu121 documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
[Torch Distributed Elastic â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/distributed.elastic.html)
[TorchElastic Kubernetes â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/elastic/kubernetes.html)
[elastic/kubernetes at master Â· pytorch/elastic (github.com)](https://github.com/pytorch/elastic/tree/master/kubernetes)
[Kubeflow Pipelines â€” PyTorch/TorchX main documentation](https://pytorch.org/torchx/latest/pipelines/kfp.html)
[TorchX â€” PyTorch/TorchX main documentation](https://pytorch.org/torchx/latest/)
[Kubeflow Pipelines â€” PyTorch/TorchX main documentation](https://pytorch.org/torchx/latest/pipelines/kfp.html)

ç›‘æ§ Pytorch

[Metrics â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/elastic/metrics.html)

Trainer

[Distributed training with ğŸ¤— Accelerate (huggingface.co)](https://huggingface.co/docs/transformers/accelerate)

- [PyTorch åˆ†å¸ƒå¼è®­ç»ƒå®ç°(DP/DDP/torchrun/å¤šæœºå¤šå¡) - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/489011749)
- [Pytorch - åˆ†å¸ƒå¼è®­ç»ƒæç®€ä½“éªŒ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/477073906)
- [PyTorchåˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€--DDPä½¿ç”¨ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/358974461)
- [å¼€æºä¸€ä¸ª PyTorch åˆ†å¸ƒå¼ï¼ˆDDPï¼‰è®­ç»ƒ mnist çš„ä¾‹å­ä»£ç  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/463842164)
- [Distributed Data Parallel â€” PyTorch 2.1 documentation](https://pytorch.org/docs/stable/notes/ddp.html)
- [Machine Learning as a Flow: Kubeflow vs. Metaflow | by Roman Kazinnik | Medium](https://roman-kazinnik.medium.com/machine-learning-as-a-flow-kubeflow-vs-metaflow-75f65bd251ec)
- [Ring Allreduce - ç®€ä¹¦](https://www.jianshu.com/p/8c0e7edbefb9)
- [GPUé«˜æ•ˆé€šä¿¡ç®—æ³•â€”â€”Ring Allreduce](https://picture.iczhiku.com/weixin/message1570798743118.html)
- [Reduced ring - Wikipedia](https://en.wikipedia.org/wiki/Reduced_ring)
- [Machine Learning Distributed: Ring-Reduce vs. All-Reduce | by Roman Kazinnik | Medium](https://roman-kazinnik.medium.com/machine-learning-distributed-ring-reduce-vs-all-reduce-cb8e97ade42e)
- [ã€è½¬è½½ã€‘ Ring Allreduce (æ·±åº¦ç¥ç»ç½‘ç»œçš„åˆ†å¸ƒå¼è®¡ç®—èŒƒå¼ -------------- ç¯å½¢å…¨å±€è§„çº¦) - Angry_Panda - åšå®¢å›­](https://www.cnblogs.com/devilmaycry812839668/p/12446933.html)
- [ddp å¤šå¡è®­ç»ƒtorch è®°å½•_torch ddp å¡æ­»-CSDNåšå®¢](https://blog.csdn.net/weixin_43850253/article/details/131706419)
- [pytorchå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒç®€è¦åˆ†æ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/159404316)
- [Distributed data parallel training in Pytorch](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)
- [Pytorchä¸­çš„Distributed Data Parallelä¸æ··åˆç²¾åº¦è®­ç»ƒï¼ˆApexï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/105755472)
- [Pytorch åˆ†æ•£å¼è¨“ç·´ DistributedDataParallel â€” å¯¦ä½œç¯‡ | by æè¬¦ä¼Š | è¬¦ä¼Šçš„é–±è®€ç­†è¨˜ | Medium](https://medium.com/ching-i/pytorch-%E5%88%86%E6%95%A3%E5%BC%8F%E8%A8%93%E7%B7%B4-distributeddataparallel-%E5%AF%A6%E4%BD%9C%E7%AF%87-35c762cb7e08)
- [Multi-GPU training â€” PyTorch Lightning 1.4.9 documentation](https://pytorch-lightning.readthedocs.io/en/1.4.9/advanced/multi_gpu.html)
- [Deepspeed å¤§æ¨¡å‹åˆ†å¸ƒå¼æ¡†æ¶ç²¾è®² - å“”å“©å“”å“© bilibili](https://www.bilibili.com/video/BV1mc411y7jW/?spm_id_from=333.1007.tianma.10-4-38.click&vd_source=f0545eb2f2f0269a5a9941436ba53b7d)

## å‚è€ƒèµ„æ–™

[^1]: [Getting Started with Distributed Data Parallel â€” PyTorch Tutorials 2.2.0+cu121 documentation](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
